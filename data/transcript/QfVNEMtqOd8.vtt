WEBVTT

00:00.000 --> 00:12.320
Good afternoon, and welcome to the inaugural webinar of the webinar series that's a collaboration

00:12.320 --> 00:17.280
between the National Institutes of Statistical Sciences and the Federal Committee on Statistical

00:17.280 --> 00:24.000
Methodology, AI and Federal Government Uses, Potential Applications, and Issues.

00:24.000 --> 00:29.760
This is a new webinar series that we're collaborating on that's designed to offer something for

00:29.760 --> 00:37.120
everybody, every federal staff and supervisors at all levels, various levels of experience in AI.

00:38.240 --> 00:43.280
We plan to feature use cases that can span agencies and departments and disciplines,

00:43.840 --> 00:50.000
and we're going to be presenting AI use cases by modalities like text imaging and also for

00:50.000 --> 00:58.160
challenges like targeted webinars for examining the implications of AI for ethics and confidentiality

00:58.160 --> 01:03.680
and privacy and organizational barriers. And we do hope with this webinar series to

01:04.800 --> 01:10.320
push ahead and see how the cutting-edge AI enhancements may be available for all of us

01:10.320 --> 01:21.440
in the years ahead. So with that, I'm going to turn it over to David Madison for opening remarks.

01:22.000 --> 01:29.360
I'll introduce David here. Dave is a director of NIS, associate professor and associate department

01:29.360 --> 01:35.280
chair of statistics and data science at Cornell University. He is currently the founding and editor

01:35.280 --> 01:41.200
chief for data science and science and an associate editor for the Journal of Econometrics and

01:41.200 --> 01:48.720
Statistica Sinica. He is lead PI and director for many NSF-funded projects. He is an officer for the

01:48.720 --> 01:53.520
business and economic statistics section of the American Statistical Association and a member of

01:53.520 --> 02:00.720
the Institute of Mathematical Statistics and an international biometric society. Thank you, Dave.

02:01.920 --> 02:06.480
Thank you so much, Jennifer. Welcome, everyone. This is the opening event in the new AI and

02:06.480 --> 02:11.440
federal government series, and more events will be announced soon, and we hope to culminate this

02:11.440 --> 02:17.120
series with an in-person event in the D.C. area this spring. So this timely series arose from

02:17.120 --> 02:22.000
collaboration between the FCSM and NIS, the National Institute of Statistical Sciences,

02:22.000 --> 02:26.960
especially out of the NIS Government Affiliates Committee and in partnership with NIS's long-term

02:26.960 --> 02:33.120
collaborations with NASS and USDA. NIS is a 30-year-old research nonprofit founded by the

02:33.120 --> 02:38.400
leading statistical societies at the time. Its mission is to seed, foster, and deliver cutting

02:38.400 --> 02:42.800
edge research in the statistical and data sciences, and that's why we're here today.

02:43.520 --> 02:50.240
NIS aims to cut across disciplinary boundaries and sector boundaries. NIS aims to, in general,

02:50.240 --> 02:55.120
partner and support to strengthen ongoing developments across our entire profession,

02:55.920 --> 02:59.600
and in particular to lead when there's important gaps or new opportunities arise.

03:00.560 --> 03:05.680
NIS has a vibrant affiliates program with 50-plus members. If you haven't heard of NIS before,

03:05.680 --> 03:11.600
I'd encourage you to check out NIS.org. We have new programs launching in the coming months,

03:11.600 --> 03:17.680
including an upcoming themed semester research program, joint postdoc and pre-doc programs,

03:17.680 --> 03:23.200
and a new NIS co-laboratory for seeding new projects and long-term collaborations. So I

03:23.200 --> 03:27.120
want to thank all of our organizers, our distinguished panelists, and then the staff

03:27.120 --> 03:31.120
for all their efforts putting this together. Thank you very much. Back to you, Jennifer.

03:32.720 --> 03:38.720
Thank you, Dave. And as a member of the Federal Committee on Statistical Methodology,

03:38.720 --> 03:43.040
I'll just say that the Federal Committee on Statistical Methodology, or FCSM, is an

03:43.040 --> 03:50.400
inter-agency committee dedicated to improving the quality of the federal statistics. I think

03:50.400 --> 03:55.440
it started off being focused on the federal statistical system and the primary federal

03:55.440 --> 04:01.040
statistical agencies, but now we are very committed to improving the quality of all federal statistics.

04:01.920 --> 04:08.320
So it's an exciting time for AI. I think many of you may have seen that the

04:09.040 --> 04:14.000
White House issued a new executive order yesterday, which I think we're going to hear more about that

04:14.960 --> 04:22.240
coming up. So what we're going to do today is have introductory perspectives from each of our five

04:22.240 --> 04:28.960
panelists. I will be posing a set of questions to these panelists afterwards, and there will be time

04:28.960 --> 04:34.080
at the end for audience questions. And in the interest of just moving ahead with it,

04:34.080 --> 04:40.320
I'm going to introduce our first panelist, Chris Markham. Chris holds the current role of Senior

04:40.320 --> 04:45.840
Statistician and Senior Science Policy Analyst in the White House Office of Management and Budget,

04:45.840 --> 04:50.480
Office of the Chief Statistician of the United States. Previously, he served as the Assistant

04:50.480 --> 04:55.920
Director for Open Science and Data Policy at the White House Office of Science and Technology Policy.

04:55.920 --> 05:00.560
In that capacity, he advanced the Bidas-Harris administration priorities on restoring trusting

05:00.560 --> 05:05.760
government through scientific integrity and evidence-based policymaking and delivering

05:05.760 --> 05:11.520
a more equitable, accessible return on the American public's investment in federal research. Chris?

05:14.400 --> 05:21.360
Thank you. Thank you so much, Jennifer, and thanks, of course, to NIS and to SCSM for inviting the

05:21.360 --> 05:30.960
Office of Chief Statistician to provide some perspective on the sort of very, very fast pace

05:30.960 --> 05:36.480
of artificial intelligence as it's interfacing with the federal statistical system in this day

05:36.480 --> 05:45.120
and age. As Jennifer mentioned, yesterday, President Biden signed the executive order on the safe,

05:45.120 --> 05:50.880
secure, trustworthy development and use of artificial intelligence. The order represents

05:50.880 --> 05:57.840
really an enormous breadth of government actions that promote responsible entry into the artificial

05:57.840 --> 06:06.240
intelligence space, not just for use in the government, but really across society and in

06:06.240 --> 06:17.600
each of the primary sectors that have equity in artificial intelligence. I thought it would be

06:17.600 --> 06:23.440
a good idea to level the playing field in my comments, my top-level comments, in highlighting

06:23.440 --> 06:28.800
that the executive order sets forth consistent definitions of artificial intelligence and

06:28.800 --> 06:34.400
artificial intelligence models really so that we are all understanding what we're talking about

06:34.400 --> 06:41.760
because there have been many definitions out in the field, and often those definitions are speaking

06:41.760 --> 06:48.960
past each other, and so we thought that it would be good to anchor the terms used in the EEO.

06:49.600 --> 06:57.200
The term artificial intelligence, or AI, is used in the EEO as having the same meaning set forth

06:57.200 --> 07:03.840
in the AI and Government Act of 2020, and that's a machine-based system that can, for a given set of

07:03.840 --> 07:10.400
human-defined objectives, make predictions, recommendations, or decisions influencing real

07:10.400 --> 07:17.760
or virtual environments. AI systems use machine and human-based inputs to perceive real and

07:18.960 --> 07:24.720
virtual environments, abstract such perceptions into models through analysis in an automated

07:24.720 --> 07:30.880
manner, and use model inference to formulate options for information or action. The EEO goes

07:30.880 --> 07:38.800
on to also define the term AI model in a more expansive way to mean a component of an information

07:38.800 --> 07:45.040
system that implements that artificial intelligence technology and then uses computational

07:45.040 --> 07:51.520
statistical or machine learning techniques to produce some outputs from a given set of human

07:52.080 --> 07:57.600
supervisor unsupervised inputs really. Now, because the definition of AI model hinges on the

07:57.600 --> 08:04.000
deployment of the technology itself, it protects a bit against the scope creep by excluding the

08:04.000 --> 08:08.960
traditional deductive statistical methods that we all have used for our inference.

08:09.760 --> 08:15.120
Now, there are important equities in the federal statistical system in the executive order. First

08:15.120 --> 08:21.280
most is how the executive order reaffirms that the federal government will ensure that the collection,

08:21.840 --> 08:28.960
use, and retention of data is done lawfully, is secure, and mitigates against privacy and

08:29.040 --> 08:34.400
confidentiality risks that we care quite dearly about in the federal statistical system.

08:35.200 --> 08:39.600
The executive order also instructs agencies to make headway into the development of privacy

08:39.600 --> 08:45.760
enhancing technologies to ensure that America's protected data remains secure from those risks

08:45.760 --> 08:52.960
and from the risk of unlawful disclosure and use by AI systems. Provisions are fully aligned with

08:52.960 --> 08:58.560
Title III of the Evidence Act of 2018 and with OMB's responsibilities for promulgating

08:58.560 --> 09:04.240
regulations for secure access to confidential statistical data. Our other equities for the

09:05.440 --> 09:10.880
federal statistical system from the executive order include how commercially available information

09:10.880 --> 09:18.240
is obtained, quality controlled, shared, and used by federal agencies, and the EO directs OMB to

09:18.240 --> 09:23.600
consult with the Federal Privacy Council as well as the Interagency Council on Statistical Policy

09:23.600 --> 09:28.720
to inform potential guidance to federal agencies on ways to mitigate privacy and

09:28.720 --> 09:33.600
confidentiality risks from agencies' activities related to commercially available information.

09:33.600 --> 09:40.800
So this is data that we might buy and then use in an AI model and that has to conform to the same

09:40.800 --> 09:47.120
standards that we would normally use for information quality control and dissemination.

09:48.400 --> 09:54.000
We know that the federal system will lead in the safe, secure, and trustworthy development and use

09:54.000 --> 09:58.880
of AI. Recognized statistical units and agencies and the principle of statistical agencies of the

09:58.880 --> 10:04.000
federal statistical system have subject matter experts in data collection, curation, methods

10:04.000 --> 10:08.720
development, analysis, and thought leaders in how emerging technology can best serve our constituents

10:08.720 --> 10:14.400
and users. We're looking forward to working with agencies in the system to implement the EO within

10:14.400 --> 10:19.040
their authorities and capacities and to leverage that expertise. I'm looking forward to the Q&A.

10:23.760 --> 10:33.600
Thank you, Chris. Our next panelist is Dr. Xiuming He. Dr. He joined Washington University in July

10:33.600 --> 10:39.200
2023 as the inaugural chair of the Department of Statistics and Data Science. Previously,

10:39.200 --> 10:44.000
he served as the HC Carver Collegiate Professor of Statistics at the University of Michigan.

10:44.400 --> 10:49.200
He is a renowned leader in the fields of robust statistics, quantile regression, Bayesian inference,

10:49.200 --> 10:54.080
and post-selection inferences. He is also a proponent of the interdisciplinary research

10:54.080 --> 11:03.920
in data science. Dr. He? Thank you. Thank you, Jennifer, for the nice introduction and also I'm

11:03.920 --> 11:11.760
grateful for the opportunity to be able to share my comments. Let me first make sure I have the

11:12.160 --> 11:19.680
four screens shared. Okay, great. For those of you who are at my age, you may still remember AI

11:20.240 --> 11:26.720
from science fiction before the powerful computers became a reality. When I was an assistant

11:26.720 --> 11:32.400
professor actually at the University of Illinois at Urbana-Champaign, I was among the first actually

11:32.400 --> 11:40.560
to use MoZack, which is the first web browser developed there. Since then, computer power and

11:41.360 --> 11:48.560
the internet have given a new life to AI. Now AI is being talked about everywhere in the public

11:48.560 --> 11:58.320
and the private sector. Obviously, AI depends on logic and algorithms and AI advances most

11:58.880 --> 12:06.800
when machines can learn from data and experience. Many in the world actually have considered AI as a

12:07.520 --> 12:12.400
transformative force in the digital age. It's really becoming a very hot topic.

12:12.400 --> 12:19.920
Just a few days ago, Oliver Chingaya, who is actually the director of African Center for

12:19.920 --> 12:25.440
Statistics for the UN Economic Commission, also a vice president for the International Statistical

12:25.440 --> 12:32.160
Institute, wrote a very nice piece entitled, The Future of AI in Statistics in Africa is the

12:32.160 --> 12:39.840
continent and ready. And I also want to mention Singapore, a small island nation in Southeast

12:39.840 --> 12:48.080
Asia that's pursuing national AI strategy in a journey to smart nation. And their action book

12:48.080 --> 12:54.080
made it very clear that the government and business will use AI to generate economic gains and

12:54.080 --> 13:00.960
improve lives. There's no doubt that the world looks to the United States federal government as

13:00.960 --> 13:08.240
a leader in the AI use. The National Artificial Intelligence Advisory Committee, also known as

13:09.120 --> 13:17.760
NAIAC, just issued its year one report a few months ago and their executive summary starts

13:17.760 --> 13:24.080
with the United States is facing a critical moment, a moment of both significant opportunity

13:24.080 --> 13:31.840
and complexity. Now, obviously I wish that the NAIAC has more statistical representation.

13:33.600 --> 13:40.640
I'm not going to repeat what Chris has already talked about in terms of what AI is, because

13:40.640 --> 13:47.360
the question I think right now is not what AI is and whether we need to embrace AI. The question is

13:47.440 --> 13:55.200
how to turn AI into a powerful and responsible machine intelligence in federal government.

13:56.320 --> 14:03.280
I would view AI as a very sharp knife. If you use it right, it's incredibly useful.

14:04.320 --> 14:11.360
If not, then you can get hurt actually badly. Coming back to statistics, I would say the

14:11.360 --> 14:17.200
statistical principles and methods are more closely related to what I call human intelligence.

14:17.920 --> 14:23.680
Statistical thinking and statistical approaches focus more on understanding, information sharing,

14:23.680 --> 14:30.160
experience sharing, and idealized models. When I say understanding, I really mean understanding

14:30.160 --> 14:35.840
at different levels, including data collection, including who might have responded to a survey,

14:35.840 --> 14:42.480
what might have been learned from domain experts, and when we need to take corrective actions,

14:42.480 --> 14:49.280
and so on. My main point today is to say that integration of machine intelligence,

14:49.280 --> 14:54.400
as represented by AI, and human intelligence, where statistics is really a certain part of it,

14:54.400 --> 15:02.800
is what we really need. AI is developing really rapidly to mimic certain parts of human intelligence.

15:03.760 --> 15:09.280
Over time, more and more human intelligence will be integrated into AI, so I believe the

15:09.280 --> 15:14.640
statistics community can help this process, even though it's often the case that at the end,

15:14.640 --> 15:19.440
very little credit is given back to the statisticians. If you think of a statistical

15:19.440 --> 15:26.400
part of human intelligence as a pool of 100 different seeds, as 10 of them get absorbed

15:26.400 --> 15:33.280
into AI, we will be and should be able to generate 20 more, because human intelligence

15:33.280 --> 15:37.760
has unlimited potential, especially when embraced machine intelligence to help us

15:37.760 --> 15:45.040
think further and do more. Why is statistics so important? Why is human intelligence so important?

15:45.040 --> 15:51.680
Because machine intelligence itself is limited to what is designed to offer. When it comes to ethics,

15:52.400 --> 15:58.640
bias, discrimination, accountability, and so on, machines are usually unaware of the causes

15:58.640 --> 16:02.400
and consequences, so public trust will then become a major issue.

16:04.400 --> 16:11.440
While we often hear about epic failures of AI, and we are constantly amazed at what AI can do for us,

16:12.160 --> 16:18.000
so I would call for more emphasis on human intelligence, including foundational,

16:18.000 --> 16:23.440
as well as interdisciplinary research and training, built on well-researched statistical

16:23.440 --> 16:28.960
principles and methods. At the same time, more openness to machine intelligence to promote

16:28.960 --> 16:35.920
better cooperation of statistical principles into AI. I think that this is the responsibility

16:35.920 --> 16:40.320
of everyone in the statistics community, whether you work for the federal government or not.

16:41.520 --> 16:46.640
Let me end my remarks by saying that human intelligence and machine intelligence must

16:46.640 --> 16:53.120
work together hand in hand as we will dramatically quicken our journey to smart nations,

16:53.120 --> 16:59.280
and machine can get smarter and more responsible, and humans get even smarter. Thank you for listening.

17:04.400 --> 17:10.880
Thank you. I do like the image of humans getting even smarter, rather than some of the dystopias

17:10.880 --> 17:22.720
where we see humans getting even less smart. Our third panelist is Dr. Mukherjee. Dr. Mukherjee is

17:22.720 --> 17:28.640
the John D. Kaplash Distinguished University Professor and Chair of the Department of

17:28.640 --> 17:33.840
Biostatistics, Professor, Department of Epidemiology, and the Professor of Global Health

17:33.840 --> 17:40.400
at all at the University of Michigan's School of Public Health. It's a lot. She also serves

17:40.400 --> 17:44.880
as the Associate Director for Quantitative Data Sciences at the University of Michigan

17:44.880 --> 17:50.240
Rogo Cancer Center and has been engaged with the University of Michigan Precision Health,

17:50.240 --> 17:56.640
an institution-wide presidential initiative for the last decade in various roles. As of 2023,

17:56.640 --> 18:02.000
she holds the position of Assistant Vice President for Research for Research Data Services Strategy

18:02.000 --> 18:06.880
and is the founding director of the University of Michigan Summer Institute on Big Data.

18:06.880 --> 18:11.680
Her research interests include statistical methods for analysis of electronic health records,

18:11.680 --> 18:17.760
studies of gene-environment interactions, Bayesian methods, and shrinkage estimation

18:17.760 --> 18:23.600
with strong collaborative areas, mainly in cancer, cardiovascular diseases, reproductive health,

18:23.600 --> 18:32.720
exposure sciences, and environmental epidemiology. Ravar, go ahead. Thank you so much, Jennifer,

18:32.720 --> 18:39.280
and thanks to Ness and FCSM for giving this opportunity to speak on this very important

18:39.280 --> 18:48.000
issue, which is tiering through humanity. I did not prepare slides. As you saw that

18:48.000 --> 18:54.480
I just started as AVP of research data strategy. I have been in meetings about AI and how to

18:54.480 --> 19:00.000
navigate AI and the challenges associated with AI across the University of Michigan campus.

19:00.800 --> 19:06.320
What I say today is probably going to be more relevant for federal statisticians working in

19:06.320 --> 19:13.040
the health sector because I sit in a school of public health and most of my work is around

19:13.040 --> 19:21.840
health data. I just want to start by mentioning that the first paper on AI by Alan Turing was

19:21.840 --> 19:29.040
way back in 1950s in terms of can machines think? This is a seminal paper. Many of us have

19:29.760 --> 19:36.800
read and heard about this paper. Why are we talking about it today? Because since last year,

19:36.800 --> 19:44.480
generative AI has touched every possible core of human life. Somebody, a historian of science,

19:44.480 --> 19:50.320
was asking me that whether I should think myself to be fortunate that I lived through a pandemic

19:50.880 --> 19:58.720
discovery of the internet and also generative AI. This is actually a confluence of factors

19:58.720 --> 20:06.560
which define our generation in a very, very profound way. I would like to really think from

20:06.560 --> 20:12.960
a humanitarian perspective here because I do think that we have lots and lots of good models,

20:12.960 --> 20:19.120
certain technology. I really love theater, so I want to start my remarks with a quote

20:19.840 --> 20:28.000
from Uncle Vania, a play by Anton Chekhov. This quote actually is coming from 1898,

20:28.560 --> 20:36.160
lots and lots of years ago, but the fundamental question for scientists and educators is that

20:36.240 --> 20:44.160
will those people who live after us, in the course of 100 or 200 years, will they remember

20:44.160 --> 20:51.840
and speak kindly of us? As we think about AI, particularly sitting in a school of public health,

20:51.840 --> 21:01.280
I have to remember that my job is not just to predict but to prevent. I think it's extremely

21:01.280 --> 21:08.800
important to think about interpretable AI, and this is not a new concept, and also remember

21:08.800 --> 21:16.640
our social responsibility as we use it. I am very enthusiastic about what we are seeing in

21:16.640 --> 21:22.240
terms of case studies. For example, I work a lot with electronic health record data,

21:22.800 --> 21:30.240
and we use a lot of heterogeneous data in order to define a phenotype, and that involves text data,

21:30.240 --> 21:37.440
imaging data, voice messages left by patients, and it takes a lot of effort by statisticians to

21:37.440 --> 21:43.920
come up with integration of these heterogeneous data sources to map into a phenotype, and for

21:43.920 --> 21:50.640
each disease we have different kind of algorithms and NLP techniques, but what I have seen in the

21:50.640 --> 21:58.400
initial phases of even defining smoking phenotype in the electronic health record using generative AI

21:58.400 --> 22:05.600
tools has been so much more accurate than our existing algorithm. I also saw that there are

22:05.600 --> 22:12.160
many more advances in terms of even matrix factorization, and you can imagine how many

22:12.160 --> 22:18.880
times matrix matrices are multiplied in modern computing every day, and a fraction of reduction

22:18.880 --> 22:25.600
of time in matrix factorization leads to humongous advantages in terms of computational time,

22:26.320 --> 22:33.120
but I do think that AI is not here to replace. I agree completely with my colleague, former colleague

22:33.680 --> 22:41.280
Dr. Xiuming He, who has left Michigan, but I really agree with the point that it is not here

22:41.280 --> 22:49.040
to replace human intelligence, but really as a complementary and supporting effort to human

22:49.120 --> 22:57.440
intelligence. I compare this with our knowledge and working operations before and after the

22:57.440 --> 23:03.200
advent of internet. I remember as a graduate student growing up in India, I used to remember

23:03.200 --> 23:09.360
so many formally, so many equations, and memorization was a big part of my graduate learning,

23:09.360 --> 23:15.360
and after the internet we do not remember most of this formula. We look up Wikipedia,

23:15.360 --> 23:21.760
and many times there are certain errors in certain Wikipedia pages, and we have the expertise to

23:21.760 --> 23:29.680
find that. I sort of draw that parallel with AI tools that when the AI is hallucinating,

23:29.680 --> 23:36.080
expertise will be needed, but some of the rudimentary work in terms of say report generation,

23:36.080 --> 23:43.360
data cleaning, if we do not want to do that, then it can expedite that work. So what is my

23:43.920 --> 23:50.640
concern? I am very positive about making things accelerated, making things supported, and some of

23:50.640 --> 23:57.840
the work that we do not particularly find very challenging to be eliminated and taken away from

23:57.840 --> 24:05.040
our plate. So there is more white space for creative thinking. What gets me worried about

24:05.040 --> 24:12.000
the bias in the training data sets? I grew up in India, and most of the training data, if you look

24:12.000 --> 24:20.320
at the data portfolio, is not coming from the unseen and the underrepresented. If we think about

24:20.320 --> 24:27.360
data equity, and the United States may put out a mandate for equity within the United States,

24:27.360 --> 24:34.240
but I also worry about global data equity, and I want the world data map to be represented,

24:34.240 --> 24:39.920
but I do think that many of these problems are going to be addressed. I think there is a lot

24:39.920 --> 24:46.560
of advantages in terms of AI tools for, I was working on a case study of simulating synthetic

24:46.560 --> 24:53.040
data. So in terms of simulating synthetic data, preserving privacy and confidentiality, there is

24:53.040 --> 25:00.720
a lot of literature existing in statistics, and I think that the generative AI can actually

25:00.720 --> 25:07.120
contribute to that literature quite substantially. In terms of thinking about policy simulation,

25:07.120 --> 25:13.360
what kind of scenarios of policy is reasonable? Because it can mine a lot of the text and

25:13.360 --> 25:19.840
existing content, it is coming up with a lot of interesting scenarios for policy implementation,

25:19.840 --> 25:27.520
because I work a lot on COVID models and how to do policy simulation. Automated report generation,

25:27.520 --> 25:35.120
I see a lot of advantages of automating that work. So I think that we have to remember that

25:35.200 --> 25:41.040
uncertainty quantification is a very key strength of statisticians and modelers,

25:41.040 --> 25:48.160
and predictive modeling and forecasting is very important, and it is the primary task, but we have

25:48.160 --> 25:55.120
to come up with uncertainty measures as well. And I do think that in terms of that uncertainty

25:55.120 --> 26:02.160
quantification, we can contribute a lot. I have been asking Chad GPT answers to a question and

26:02.160 --> 26:09.680
asking, how certain are you using the AI tool interactively to elicit its own uncertainty?

26:09.680 --> 26:15.520
And over time, its uncertainty estimate is changing. And I'm very, very interested in

26:15.520 --> 26:21.520
the operational what is happening under the hood. So with that, I think I'm going to be

26:21.520 --> 26:27.280
very optimistic in terms of doing more creative stuff, and I could not agree more

26:28.160 --> 26:35.040
with Xiuming that humans are probably going to become smarter because some of the other work

26:35.040 --> 26:40.400
which we do not particularly want to do would be taken away from our plate.

26:43.360 --> 26:51.120
Thank you. One thing that makes me optimistic is that people that are really active

26:51.120 --> 26:56.960
in this area are going to help us all become smarter with AI.

26:57.520 --> 27:01.920
Our next panelist is Michael Haas.

27:04.560 --> 27:09.920
Oh, are you there? Yes, I see you. Michael is the Senior Advisor for Data Access and Privacy.

27:09.920 --> 27:14.800
He is responsible for outreach and engagement with the Census Bureau's data users on issues

27:14.800 --> 27:20.080
related to the impact of privacy protection methodologies on the accessibility and usability

27:20.080 --> 27:25.200
of census data. Prior to joining the Census Bureau, Michael served as Director of Student

27:25.200 --> 27:30.400
Privacy at the U.S. Department of Education. Michael is a colleague of mine on the Federal

27:30.400 --> 27:36.400
Committee on Statistical Methodology and Chair of its Confidentiality and Data Access Committee.

27:36.960 --> 27:41.200
He has supported numerous federal government-wide initiatives related to data privacy and

27:41.200 --> 27:45.840
confidentiality and served as a privacy consultant to the Federal Commission on

27:45.840 --> 27:51.760
Evidence-Based Policymaking. Michael? Great. Thank you, Jennifer, and good afternoon, everyone.

27:52.720 --> 27:58.640
So, advances in technology often lead to new and emerging privacy and ethical concerns,

27:59.360 --> 28:05.520
and strategies that we can develop and solutions that we can use to mitigate

28:05.520 --> 28:12.880
these privacy and ethical concerns typically lag behind technological advances. So, we see

28:12.880 --> 28:18.880
kind of new concerns arising in the immediate aftermath of the development of a new technology

28:18.960 --> 28:24.960
or the implementation of new technology. And then over time, we see kind of the privacy concerns

28:24.960 --> 28:28.560
being identified and new solutions to address those concerns being implemented.

28:30.160 --> 28:36.480
The same is true and will be true with artificial intelligence. I think as a pattern,

28:37.200 --> 28:43.360
the more complex and opaque a technology is when it's being introduced to society,

28:43.360 --> 28:48.160
the thornier the privacy and ethical concerns can be to both identify and to navigate.

28:49.600 --> 28:55.440
In the case of AI, there are several privacy and ethical concerns that we've already identified

28:55.440 --> 29:00.160
as relevant for federal agencies and particularly for the federal statistical system.

29:01.200 --> 29:06.560
On the privacy front, there are concerns both about privacy in the context of AI-generated

29:06.560 --> 29:14.640
information and concerns about AI being used to attack or defeat the confidentiality protections

29:14.640 --> 29:19.680
of data that are generated in more traditional ways that agencies release to the public.

29:20.640 --> 29:27.200
On the AI-generated information front, when AI models are trained on confidential information,

29:27.840 --> 29:32.720
there's substantial risk of the confidential information from those training data sets

29:32.720 --> 29:39.200
kind of leaking in subsequent interactions with the AI models. Now, there are promising

29:39.200 --> 29:45.120
privacy-enhancing technologies like differential privacy and others that can address these risks

29:45.120 --> 29:50.800
of kind of confidential information leakage from training data sets, but many of these approaches

29:50.800 --> 29:56.400
are still in the early research stages and any successful adoption of these or other privacy

29:56.400 --> 30:00.800
enhancing technologies would require their inclusion at the training stage of AI development

30:01.440 --> 30:07.280
rather than as kind of a post-development privacy filter, if you will. The other major concern which

30:07.280 --> 30:13.280
I mentioned relates to AI being used in attacks on the confidentiality protections of data that

30:13.280 --> 30:19.280
federal agencies release to the public. We know that any public release of information that's

30:19.280 --> 30:24.720
derived from a confidential data source is going to reveal or leak a tiny bit of confidential

30:24.720 --> 30:29.760
information in the process. That's inevitable. There's no such thing as perfect de-identification

30:30.720 --> 30:38.880
and agencies have to select and implement disclosure avoidance mechanisms to manage

30:38.880 --> 30:43.280
that disclosure risk in the data that they publish, and in doing so, they have to

30:43.280 --> 30:48.240
navigate a very complex trade-off between confidentiality protection, data accuracy,

30:48.240 --> 30:56.000
and data availability. As new kind of attack vectors on these confidentiality protections

30:56.000 --> 31:01.760
are identified and emerge, it throws that balance off and agencies have a difficult time

31:02.400 --> 31:07.840
identifying exactly what the existing level of disclosure risk for their data assets are

31:08.480 --> 31:13.040
and compensating with additional or improved confidentiality protections on the data they're

31:13.040 --> 31:19.040
releasing. Over the past several years, we've seen kind of new and emerging attack vectors

31:19.040 --> 31:24.480
on kind of traditional disclosure limitation methods through things such as database reconstruction,

31:25.280 --> 31:31.760
through the increasing use of convex optimization technologies on publicly released data,

31:32.640 --> 31:39.600
and that makes agencies' jobs harder to protect the data. There is concern that AI could be

31:39.600 --> 31:45.680
successfully leveraged in new and emerging forms of attack on confidentiality protections, so we

31:45.680 --> 31:51.120
are going to have to be diligent in kind of figuring out, okay, how can existing and emerging

31:51.120 --> 31:58.480
disclosure limitation techniques be implemented such to limit the vulnerability of these

31:58.480 --> 32:05.760
mechanisms to AI-based attacks? On the ethical front, there are also concerns. There's concerns

32:05.760 --> 32:11.520
about data quality, which have been mentioned already, transparency, and data integrity.

32:12.720 --> 32:19.440
On the data quality front, the more that AI exists as a black box, the less confidence we can have

32:19.440 --> 32:25.040
in our ability to know what the limitations of our data are. As statisticians, it's important

32:25.040 --> 32:31.120
for us to know how much we can rely on the data that we are collecting and using, so to the extent

32:31.120 --> 32:36.560
that there are inaccuracies, to the extent that there is error or bias in the data that AI are

32:36.560 --> 32:41.120
generating, it's important for us to be able to identify those. That requires understanding the

32:41.120 --> 32:45.520
technologies and understanding kind of the workings of these black box algorithms in many cases.

32:46.320 --> 32:52.000
On the transparency front, a related issue is, how can we communicate these limitations in the

32:52.000 --> 32:59.120
data to our data users? We have an obligation to convey known limitations on data accuracy,

32:59.120 --> 33:04.640
known limitations in the form of bias, etc., to data users in order to inform their uses of our

33:04.640 --> 33:12.400
data. If we're using AI to generate data or to process or produce data for public consumption,

33:12.480 --> 33:17.120
and if we can't be sure that we know all of the limitations of the data, how can we

33:17.120 --> 33:23.600
successfully convey those to the general public? Lastly, and particularly kind of in the context of

33:24.480 --> 33:30.080
AI technologies like ChatGBT, where they're starting to take the place of search engines,

33:31.120 --> 33:41.200
if data users are asking these AIs to answer questions that require kind of the use of

33:41.200 --> 33:45.520
federal statistics, how can we ensure the integrity of the information that's being

33:45.520 --> 33:52.480
presented as federal statistics? How can we make sure that the data that these AIs are serving up

33:52.480 --> 33:57.120
to the public if they're being drawn from federal statistics? How can we ensure that they're both

33:57.120 --> 34:02.320
authoritative and accurate? And those are kind of some thorny questions that we're going to have to

34:02.320 --> 34:07.520
consider and find solutions to. So these are just a few of the concerns on the privacy and ethics

34:07.520 --> 34:12.960
front. They're absolutely ones that merit our attention, and as Chris mentioned at the beginning

34:12.960 --> 34:18.320
of our session today, yesterday's executive order actually addresses some of these. It

34:18.320 --> 34:25.200
instructs federal agencies to research solutions to these and to related concerns. It's also likely

34:25.200 --> 34:30.640
that as AI continues to be developed and adopted, that new concerns on the privacy and ethical

34:30.640 --> 34:35.760
fronts are likely to be identified, and so we should remain diligent and we should continue

34:35.920 --> 34:41.360
to review these technologies moving forward to ensure that we're able to stay kind of current on

34:42.080 --> 34:45.840
how we identify and address these privacy and ethical concerns. Thanks.

34:48.960 --> 34:56.160
Thank you, Michael. Before I introduce Nancy, I wanted to alert everybody to the Q&A tab at

34:56.160 --> 35:01.920
the bottom of your screen. We have a couple of questions, and so after each of the panelists

35:01.920 --> 35:08.320
has had their chance to provide their perspectives, I'm going to be asking a handful

35:08.320 --> 35:13.600
of moderated questions, and then toward the end of the webinar we will go through the participant

35:13.600 --> 35:19.760
questions. So your question should be answered then. If not, we will figure out a way to find

35:19.760 --> 35:29.120
an answer for you. So our last panelist who will be giving her opening remarks is Dr. Nancy Potok.

35:30.000 --> 35:35.760
Nancy Potok has over 38 years of senior-level experience in the public policy arena within

35:35.760 --> 35:42.240
the public, private, and nonprofit sectors. Dr. Potok is a visiting fellow at RTI International

35:42.240 --> 35:48.240
and visiting scholar at NYU. She previously served as the chief statistician of the United States

35:48.240 --> 35:54.240
in the executive office of the president and the deputy director and was the

35:54.480 --> 36:05.360
chief operating officer of the U.S. Census Bureau. She was the deputy undersecretary for economic

36:05.360 --> 36:10.160
affairs at the U.S. Department of Commerce and senior vice president of North at the University

36:10.160 --> 36:15.280
of Chicago. She served as a commissioner on the U.S. Evidence-Based Policymaking Commission

36:15.280 --> 36:20.160
and currently chairs and serves on several boards of nonprofit organizations and academic

36:20.160 --> 36:25.520
institutions, including this. She is a fellow of the National Academy of Public Administration

36:25.520 --> 36:35.040
and the American Statistical Association. Nancy. Thanks, Jennifer, and thank you to Ness and FCSM

36:35.040 --> 36:43.200
for having this webinar series. I'm going to share my screen. I do have a couple of slides that I'd

36:43.200 --> 36:54.800
like to use to help the conversation along, so let me get this up. Let's see. I guess I have to

36:54.800 --> 37:08.160
start the slideshow somewhere here. There we go. Yeah, so I'll just move to the next slide. What

37:08.160 --> 37:17.120
I want to talk about actually is a more practical application. I'm going to bring it down a little

37:17.120 --> 37:28.880
bit to ground level here in terms of thinking about people's desire to do AI projects themselves

37:28.880 --> 37:35.680
and how you go about doing that in government successfully. I think there are several things

37:35.680 --> 37:41.840
to consider. We're talking about some of the big ideas, some of the big considerations,

37:41.840 --> 37:50.560
and what I'd like to do is talk about, let's see if I can advance my slides, some of the elements

37:50.560 --> 37:57.600
of a successful AI approach, especially if you're in government. The technical fundamentals,

37:57.680 --> 38:06.880
obviously, are key, and I think there's a lot of help out there on what are the technical

38:06.880 --> 38:12.400
fundamentals? How do you actually put these algorithms together? I think the webinar series

38:12.400 --> 38:18.720
will do a much deeper dive into this. Sometimes what people want to do isn't as complicated

38:18.720 --> 38:26.720
as it seems. A lot of it, again, depends on some of these data quality issues, what you're starting

38:26.720 --> 38:34.960
with, what you're trying to accomplish, but I think if you just jump in on one particular thing,

38:34.960 --> 38:42.240
even if you're not in management or leadership position in your organization, you still have

38:42.240 --> 38:48.240
to fit within an overall AI strategy. I think some of the panelists who came before me were

38:48.960 --> 38:58.240
laying out these strategy and policy types of issues that whatever you're doing, however big

38:58.240 --> 39:05.520
or small your project is, really has to fit in that context under the umbrella of the executive

39:05.520 --> 39:12.880
order if you're in government. The other thing to consider, too, is your organizational culture,

39:13.760 --> 39:19.680
because if the organization isn't ready to accept some of the things that you want to do,

39:19.680 --> 39:25.920
you could have a heck of a time moving your project forward. Then what is the governance

39:25.920 --> 39:33.360
structure? I think for many people in government, it's not so much that you can't conceive of a

39:33.360 --> 39:39.680
project that you don't want to innovate. It's how do you get things done through the governance

39:39.680 --> 39:45.840
structure? Now that we have chief data officers, there's CIOs, you need infrastructure.

39:47.040 --> 39:52.960
That might be your biggest barrier, actually. While you learn about AI, you have to keep these

39:52.960 --> 39:59.920
other environmental things in your mind if you're really going to get this done. Then I think Michael

39:59.920 --> 40:04.640
spent some time talking about responsible leadership and the ethical framework within

40:05.280 --> 40:10.400
which you work. There are equity issues. There's data quality issues. There's many things you have

40:10.400 --> 40:20.320
to consider. I do want to just do a very short deep dive into this, but I strongly recommend

40:20.320 --> 40:25.360
that as you're considering what you want to do in your agency, whether it's a research

40:25.360 --> 40:31.920
project or you think you want to buy something off the shelf or you're ready to launch,

40:31.920 --> 40:39.680
you really need to create a roadmap before you start. These are just my suggestions based on my

40:39.680 --> 40:45.840
own experience and the experience of lots of other people that I've interacted with across government

40:46.480 --> 40:56.720
who are working on AI projects. That is you start by having a value proposition. Why are you doing

40:56.720 --> 41:03.520
this? What is it going to contribute? I think that goes back a little bit to what Braumer was

41:03.520 --> 41:09.120
saying about what are people in the future going to think about this? Where is the value? Why?

41:09.760 --> 41:14.400
You start with that. It needs to tie into whatever the mission of your agency is

41:16.080 --> 41:24.400
and whatever you're trying to accomplish. Why does that matter? How are you going to,

41:25.120 --> 41:29.920
the third thing, create a narrative that explains what you're trying to do so you can get some

41:29.920 --> 41:36.320
support in your agency for doing this and get a supporting coalition together, whether it's

41:36.320 --> 41:41.200
people in other agencies or in other parts of the agency where you need to get your hands on the

41:41.200 --> 41:46.560
data, you need help with the infrastructure, you're going to need some money and people.

41:46.560 --> 41:54.080
These are just very practical considerations, but if you don't think about this, your AI project

41:54.080 --> 42:01.200
may be very well conceived, but it's not going to go anywhere. Then how do you

42:03.520 --> 42:10.320
set up maybe a pilot project so that you can have a deliverable and show some progress,

42:10.320 --> 42:17.440
communicate your vision, get more buy-in? I think a lot of people set out being just overwhelmed

42:18.480 --> 42:22.960
because they're trying to, as they say, boil the ocean. The scope of what you're trying to do is

42:22.960 --> 42:29.120
just way too big. If you really want to be successful, start with little things that you

42:29.120 --> 42:36.240
can do that will incrementally advance your research or your particular project that you're

42:36.240 --> 42:44.960
trying to do. It doesn't mean go super slow or don't take any risks, but I think if you have a

42:44.960 --> 42:51.440
smaller scope, if things aren't working out very well, you have something that's manageable that

42:51.440 --> 42:58.320
you can course correct and think about some of these other issues. I think one of the big

42:58.320 --> 43:05.680
problems that we see is that people kind of conflate their ability to get things done in

43:05.680 --> 43:12.400
the AI arena, the technical problems and the adaptive challenges. I think most people actually

43:12.400 --> 43:20.160
don't run into huge problems. They're kind of fun problems, the technical ones. You can find people

43:20.160 --> 43:25.680
who have the technical background, whether they're inside the agency in an academic setting that you

43:25.680 --> 43:32.880
want to partner with, an organization like NISS who can bring in experts. You can tackle those

43:32.880 --> 43:39.120
technical problems, but if you don't think about the adaptive challenges, sort of where do the

43:39.120 --> 43:48.080
people come into this? How are we having to learn new ways of behaving? How is this changing attitudes

43:48.080 --> 44:00.400
or behaviors? You could really run into a lot of resistance and fear. Sometimes those adaptive

44:00.400 --> 44:06.560
challenges are actually tougher nuts to crack than the technical ones. If you're not thinking about

44:06.560 --> 44:12.560
that as you go along, you could end up with a big surprise when people kind of resist and say,

44:12.720 --> 44:21.920
you're taking away my job. This is where as a statistician or as somebody who's supporting

44:21.920 --> 44:27.760
statisticians where I've always done this work my way, now you want to do it this different way and

44:27.760 --> 44:34.720
it's going to take away my job, so I don't want you to do it. You may not hear that exactly, but

44:34.720 --> 44:40.720
you will be hearing that as a subtext. Then Michael kind of went through some of the ethical

44:40.720 --> 44:47.120
issues and so I won't spend a lot of time on this, but I think these are ethical issues in

44:47.120 --> 44:56.880
statistics and they carry right over to AI. Privacy, bias, and it is like missing data is

44:56.880 --> 45:02.240
huge in bias, so I'm glad that was brought up. Who's not included in the training data or in

45:02.240 --> 45:07.680
other data that you might be using? This idea of transparency, especially if you want to go out and

45:07.680 --> 45:19.840
buy data or buy a software product that's going to do some of this for you, so fairness ties into

45:19.840 --> 45:27.680
that. Accountability, who's responsible when you're not quite getting the results that you want?

45:27.680 --> 45:34.320
Because this is a team effort. It requires coalitions of people. You can't sort of sit

45:34.400 --> 45:41.760
alone and do an AI project, so understanding the various roles and responsibilities. Then

45:41.760 --> 45:47.840
I think the most important thing in some ways is explainability. How are you going to really

45:47.840 --> 45:53.120
maintain trust in government by being able to explain a very clear language, what it is you're

45:53.120 --> 45:59.520
doing and what the effect is? You have to not only be able to explain it to other statisticians, but

45:59.520 --> 46:04.880
to the public because you're still going to probably be relying on public data one way or

46:04.880 --> 46:10.640
the other. When you get to this and let's say you have all these other things kind of

46:12.000 --> 46:16.880
tied in a neat package, you've got the technology, you've tested your technique,

46:18.960 --> 46:25.120
you've got some support in your agency, you still have to stop and ask yourself, is this really

46:25.120 --> 46:31.120
something we should be doing? Is this something that people in the future, again, will thank us

46:31.120 --> 46:39.280
for or are we going to deeply regret this because of some of these fairness and bias issues or

46:39.280 --> 46:46.880
violating privacy or really ending up with some trust issues out there between the public and

46:46.880 --> 46:54.240
the government? It's not just can we do it, it's should we do it. If the answer is pretty iffy

46:54.240 --> 47:00.560
and you're not sure, don't do it. You have to be prepared to stop and I think that's a really key

47:00.560 --> 47:05.360
thing too. You could get very close to being able to do something and just say, you know what,

47:05.360 --> 47:12.960
we shouldn't do this from an ethical standpoint. So I'm going to stop there so we can get to some

47:12.960 --> 47:21.520
of the questions that are both the policy and kind of the statistical application and these kind of

47:21.600 --> 47:26.400
practical things that you really have to think of if you're actually going to be doing this in a

47:26.400 --> 47:35.840
government setting. Thank you, Nancy. And so like I said before, if you have questions for the panel,

47:35.840 --> 47:42.960
you can put them in the Q&A, but first we have some questions from Nissen FCSM for the panel

47:43.520 --> 47:51.120
and I forgot to say I didn't introduce myself. I'm Jennifer Parker. I'm the Director of the

47:51.120 --> 47:55.200
Division of Research and Methodology at the National Center for Health Statistics which is

47:55.200 --> 48:00.480
located within the Centers for Disease Control and Prevention and I am also on the Federal

48:00.480 --> 48:06.400
Committee on Statistical Methodology. So it's been a pleasure to hear all of the perspectives

48:06.400 --> 48:10.960
and insights from all of our panelists and you know I have a handful of questions here but I have

48:10.960 --> 48:20.160
actually even more now that I've heard everything laid out. So my first question for the panel is

48:20.160 --> 48:24.480
what are some of the considerations that government statisticians and managers should

48:24.480 --> 48:29.440
be thinking about when deciding whether to use AI for a particular use case or as part of their

48:29.440 --> 48:35.440
workflow? And I think all of you have touched on that a little bit. I do have a second question

48:35.440 --> 48:42.640
related to this that could be answered as part of this answer. What special issues arise because

48:42.640 --> 48:49.440
AI methods are not transparent which is a fundamental value of the federal statistical system

48:49.440 --> 48:59.760
and federal statistics in general. I am going to let somebody volunteer to open up

49:00.720 --> 49:02.400
and if not I will call on somebody.

49:07.600 --> 49:12.160
All right. I'd like to hear from one of our academic colleagues.

49:13.120 --> 49:17.120
Zhimei, would you like to take a chance at answering that?

49:18.960 --> 49:26.160
Yeah, I will try. I think the question is what are the considerations that government statisticians

49:26.160 --> 49:32.080
and managers should be thinking about when they are deciding whether to use AI for a particular

49:32.080 --> 49:37.520
problem. I think that the key question for me is to think about do we really understand

49:38.240 --> 49:45.680
what AI or the particular AI method we are using, how this is working, how it arrives at those

49:45.680 --> 49:54.000
conclusions and whether certain human interventions, whether out of the more part of the human

49:54.000 --> 50:00.000
intelligence, is it really needed in arriving at the solutions. So I will certainly feel

50:00.000 --> 50:06.320
uncomfortable just blindly use whatever tool is available, whatever algorithm or platform is

50:06.320 --> 50:12.880
available. If we feel comfortable, we understand what the AI is doing for that particular problem,

50:12.880 --> 50:19.440
reaching that particular conclusion, then I will say yes this is a very powerful tool that we

50:19.440 --> 50:25.680
should rely on and we may be able to get things done more efficiently and getting better insights.

50:27.200 --> 50:29.520
So that's my quick answer.

50:37.040 --> 50:40.480
And if you are muted, I think.

50:43.600 --> 50:49.520
I thought I would get through today without that. Thank you very much. I see Chris,

50:49.520 --> 50:53.120
you have your hand up. Do you have a response or a better answer?

50:55.440 --> 51:02.800
First, a concurrence with assuming on all regardless of the tools that if you don't

51:02.800 --> 51:10.240
understand the tools, you won't be using them. We often refer to this as the black box problem,

51:10.240 --> 51:14.640
where we understand pretty well the inputs, although when you are talking about inputs

51:14.640 --> 51:18.800
on the order of billions and billions of parameters, it is often very difficult.

51:19.600 --> 51:26.160
As we know, we can often end up in situations where there are really complicated interactions

51:26.160 --> 51:32.960
that we don't quite understand that come out. So it is really, I think, very important to have

51:32.960 --> 51:45.840
open standards, to have the protocols themselves to be open and for the underlying algorithms

51:45.840 --> 51:50.880
to be visible to the public and visible to interrogation. The other thing I would like

51:50.880 --> 51:58.880
to add on this is something related that Michael had talked about, which is one of these really

51:58.880 --> 52:09.600
difficult challenges with transparency here is in data quality. We often care a lot about

52:09.600 --> 52:15.680
confidentiality, obviously. One of the tradeoffs, of course, is that we want to protect confidential

52:15.680 --> 52:24.320
data. If models aren't trained on the very best in high quality data, then their outputs will reflect

52:24.320 --> 52:30.240
that. We have to really think very carefully about, and I think this is really an existential

52:30.240 --> 52:35.600
question for the federal statistical system, think very carefully about what the balance

52:35.600 --> 52:41.760
and tradeoff is between what we allow models to be trained on so that we can transparently

52:42.160 --> 52:48.720
communicate to the public the trustworthiness of the outputs when there is still a substantial

52:48.720 --> 52:55.520
disclosure risk. Very good point about the quality of the inputs.

52:58.240 --> 53:05.120
Ramar, I see you have your hand up. Yes, so I just wanted to make a couple of comments. One is that

53:05.120 --> 53:10.720
I think it would be incredibly important to collaborate with industry. So at University of

53:10.720 --> 53:18.160
Michigan, we have created a secure, our own version of UMGBT where the data inputs are actually

53:18.160 --> 53:26.320
secure. But the underlying operating system is actually in collaboration with Microsoft.

53:26.320 --> 53:32.720
So I think that industry partnership with industry is going to be very key in terms of creating

53:32.720 --> 53:40.000
secure environments where we can use all of these tools and we can lead a augmented, meaningful,

53:40.000 --> 53:45.840
purposeful life as an analyst instead of being AI driven. That's the first thing I want to mention.

53:45.840 --> 53:53.200
The second thing is that productivity. So already by my students who use actually GitHub Copilot

53:53.200 --> 54:00.320
in terms of filling with codes are producing work much faster than students who are resisting.

54:01.760 --> 54:07.360
So stack exchange is their default, but how much different is it? And so why should I be

54:07.360 --> 54:15.600
resistant to use GitHub Copilot? So for coding and translating a syntax of code from Python to R,

54:15.600 --> 54:22.880
I think this is absolutely a very good use. You still are not going to get absolutely accurate

54:22.880 --> 54:28.560
coding, but you can fix it. It's 80% correct. So your starting point, taking a Python optimization

54:28.560 --> 54:35.120
routine to R is much faster. So that gives me hope, but also concern because I feel we already

54:35.120 --> 54:45.680
produce a lot. And in the June 2023 McKinsey report says that $6.1 to $7.9 trillion dollars

54:45.680 --> 54:51.920
added to the economy because workers will be more productive. And the projection is that by 2030,

54:51.920 --> 54:58.880
30% of the work hours are going to become automated. So in academia, if now I am expecting

54:58.960 --> 55:05.760
my junior faculty to produce more, I'm going to place that as a question. And then the third

55:05.760 --> 55:13.200
thing I want to mention is sample survey and study design. Many of the assets and I think that

55:13.200 --> 55:20.240
the national treasures in government statistics are ongoing national surveys. And many times the

55:20.240 --> 55:27.280
AI tools just takes data as data, does not really respect the sampling design. So we need to really

55:27.280 --> 55:32.560
work in partnership in order to respect the study design and how data are collected. And

55:32.560 --> 55:42.880
that's very unique to our field. Thank you. Michael, I think I saw your hand up next.

55:43.520 --> 55:47.920
Yes. So I just wanted to follow up on two things that Chris had mentioned. So the first,

55:47.920 --> 55:54.240
I absolutely want to second what he was saying about how thorny the balancing act can be between

55:55.200 --> 56:02.880
ensuring data quality and data protection. And I think in the AI context, that can be even

56:02.880 --> 56:08.720
more challenging than it already is, which is not reassuring. But it's certainly something that

56:08.720 --> 56:15.280
merits a lot of thought and consideration. The second gets back at the transparency part of the

56:15.280 --> 56:20.640
initial question there. And I think Chris did a really good job of explaining the importance of

56:20.640 --> 56:28.880
algorithmic transparency as part of that, kind of moving away from the black box nature of a lot

56:28.880 --> 56:34.640
of discussions about AI. But I think it's more than just kind of like opening up the black box

56:34.640 --> 56:40.400
and having transparency algorithms. I think the other part of this that's going to be particularly

56:40.400 --> 56:47.520
challenging from a transparency front is going to be explaining to non-technical audiences,

56:47.520 --> 56:52.080
but audiences that are using the data that are produced from these algorithms,

56:52.080 --> 56:57.040
explaining to these non-technical audiences exactly what's being done to create or to process

56:57.040 --> 57:02.960
these data. Because this is a highly complex technical domain, and it's not going to be easy

57:02.960 --> 57:08.480
to explain, and particularly not going to be easy to explain to those who are not kind of well-versed

57:08.480 --> 57:15.040
in this type of algorithmic decision making. So I think we're going to have our work cut out for us

57:15.040 --> 57:18.640
on that kind of transparency front if we're going to be able to explain kind of what's

57:18.640 --> 57:21.360
being done to the data and what the implications on data use are.

57:25.600 --> 57:31.200
Making these problems even greater for the federal statistical system. Nancy?

57:32.880 --> 57:39.440
Yeah, thanks. I would like to kind of posit a different thought here, maybe a little bit of

57:39.520 --> 57:47.760
a different perspective that I don't think is being talked about much, but could be helpful.

57:47.760 --> 57:55.440
And that's really kind of in a governance slash autonomy context of thinking about the government

57:55.440 --> 58:04.000
and particularly statistical agencies doing AI. We want openness, we want transparency.

58:04.960 --> 58:12.400
The agencies also want a lot of autonomy when it comes to their methodology. And I think

58:12.400 --> 58:21.600
transparency helps with that. But if we look at some international models, for example, not

58:21.600 --> 58:28.560
technical models, but governance models. If you look at, for example, the UK, they have a really

58:29.520 --> 58:38.400
sophisticated oversight and evaluation capacity where they have what I would say an intermediary

58:38.400 --> 58:47.280
between an independent statistical function off the national statistics and the public

58:47.840 --> 58:56.640
that kind of gives the seal of approval in a non-technical way by providing this oversight.

58:56.720 --> 59:01.280
And we don't really have that in our governance structure. I don't think OMB

59:01.280 --> 59:08.640
and the chief statistician's office right now is set up to be able to do that the way that they

59:08.640 --> 59:14.960
do it in the UK, because it's tied to the White House. And so that can create trust issues as

59:14.960 --> 59:22.800
well. It's like inside the government saying, oh, trust us from the White House. And we've seen that

59:22.800 --> 59:30.800
can create problems. So I would say, maybe we should be thinking along with the technical

59:30.800 --> 59:38.640
solutions to these issues about maybe we need a different governance structure that can sort of

59:38.640 --> 59:45.920
look at the validation process, have the human intervention, review these algorithms technically,

59:45.920 --> 59:51.440
and then be able to turn around in a non-technical way and say to the public, here's what we found.

59:51.440 --> 59:59.520
We found bias or we didn't find bias. And then you don't have to reveal all of the algorithms and

59:59.520 --> 01:00:06.480
the micro data that you're using that could create privacy problems if you're using micro

01:00:06.480 --> 01:00:13.520
data for the training data, because then you have a limited group of outside objective people

01:00:13.520 --> 01:00:20.240
who can sort of give that good housekeeping seal of approval and say, no, this is actually okay.

01:00:20.240 --> 01:00:24.400
So that is not part of the conversation right now, as far as I can

01:00:24.400 --> 01:00:28.640
tell in a serious way. And I just want to put it out there as something to think about.

01:00:35.040 --> 01:00:42.160
Thank you, Nancy. I apologize. I was telling the group I was a little bit

01:00:42.160 --> 01:00:51.280
with. I had a touch of laryngitis over the weekend. So I think that's a good segue into

01:00:51.280 --> 01:01:01.680
our next question, actually. Where do you see the biggest impacts for government statistics with AI?

01:01:02.640 --> 01:01:08.800
And how can academic researchers and statistics and governments, academic researchers and

01:01:08.800 --> 01:01:13.200
government statisticians each contribute and collaborate towards the use of AI for

01:01:13.200 --> 01:01:18.000
government statistics? And I apologize for my voice. Michael?

01:01:19.120 --> 01:01:27.520
Yeah. So one, I think one low-hanging fruit that I know the Census Bureau is actively looking into,

01:01:27.520 --> 01:01:33.440
and I believe other agencies are as well, and that's using large language models to

01:01:34.320 --> 01:01:41.040
better understand what people who are coming to our website are looking for and also better

01:01:41.040 --> 01:01:48.400
being able to serve our information up to the general public in other kind of web environments.

01:01:49.200 --> 01:01:56.480
And the reason for that is so we have very particular nomenclature for our data collections

01:01:56.480 --> 01:02:04.240
and for our data products. And we refer to things as like Table P-19 or Table G-001 or things like

01:02:04.240 --> 01:02:08.960
that. We have very regimented nomenclature for the different data products that we're releasing and

01:02:08.960 --> 01:02:14.160
for the different attributes and variables that are included therein. And that's not the nomenclature

01:02:14.160 --> 01:02:20.880
that your average person looking for an official statistic is actually going to use. And so there's

01:02:20.880 --> 01:02:29.120
often kind of a translation gap in there where kind of real language questions need to be

01:02:29.120 --> 01:02:36.560
translated into their official statistics tables kind of official nomenclature and vice versa.

01:02:36.560 --> 01:02:41.440
So we're looking into large language models as a mechanism for better being able to kind of perform

01:02:41.440 --> 01:02:47.040
that translation both on the input side and on the output side to better serve data up to the

01:02:47.040 --> 01:02:56.320
people who are ultimately the consumers of it. Yeah, that's a perfect use. Nancy?

01:03:00.640 --> 01:03:07.280
Thanks. Yeah, I just wanted to mention a pilot project that I've been working on. It was one

01:03:07.280 --> 01:03:14.720
that we talked about at the recent FCSM conference and that was using natural language processing

01:03:15.280 --> 01:03:27.280
models and several algorithms to find out who's using statistical data. Going beyond kind of the

01:03:27.280 --> 01:03:38.240
downloads from the website and really being able to see by looking at scientific publications

01:03:38.240 --> 01:03:44.560
and reading the full text of the articles and then looking at reports that agencies are putting out

01:03:45.120 --> 01:03:53.920
and other types of things to actually find the unsighted data that's being used and the data

01:03:53.920 --> 01:04:00.880
assets that often don't show up just if you're looking for citations. And then making it available

01:04:02.080 --> 01:04:07.920
to both the agencies and the users to start really building coalitions around

01:04:08.880 --> 01:04:16.480
communities of data users, whether it's health data or education data or helping agencies kind

01:04:16.480 --> 01:04:24.480
of get a lot more breadth and depth by sharing data and looking at how researchers are linking

01:04:24.480 --> 01:04:31.600
data from different agencies and understanding that a lot better so the agencies can actually

01:04:31.680 --> 01:04:39.600
improve the quality of the data and start making much better connections with the user groups.

01:04:39.600 --> 01:04:48.000
And from an equity standpoint, again, see who's missing, which researchers are not using the data,

01:04:48.000 --> 01:04:57.200
which communities are missing there, and how can the agencies do more outreach. And then tying that

01:04:57.920 --> 01:05:04.800
through better kind of search functions that could be algorithmically driven the way that

01:05:04.800 --> 01:05:11.440
Amazon is so that when someone is coming to like the application portal to get access to statistical

01:05:11.440 --> 01:05:17.440
data, they can now see a lot of usage data, they can connect with other researchers and have a much

01:05:17.440 --> 01:05:24.880
fuller portfolio and of understanding, especially newer researchers can see them and agencies can

01:05:24.880 --> 01:05:33.680
see it too. So I think there's a lot of practical uses out there to really help

01:05:35.280 --> 01:05:41.120
increase the public in researchers appreciation and the value of statistical data can be

01:05:41.120 --> 01:05:50.160
demonstrated much better, which can only help the system. I really like the focus on the external

01:05:50.160 --> 01:05:56.560
users and the general public and making statistics a little bit more friendly for everybody.

01:06:00.160 --> 01:06:06.240
Thank you, Jennifer. I want to add to the one big impact, I think, for government statistics

01:06:06.240 --> 01:06:12.320
using AI is that I think the more and more government leaders will be under pressure

01:06:12.320 --> 01:06:20.000
to ask what more and how much more AI can do in federal government. And in turn, I think that

01:06:20.000 --> 01:06:27.440
people doing statistical work will be more appreciated. So that's a more optimistic view

01:06:27.440 --> 01:06:34.400
of that. And I also want to come to the collaboration between academic research statisticians

01:06:34.400 --> 01:06:38.960
and the statisticians working in the federal government. I think that obviously the statisticians

01:06:38.960 --> 01:06:43.200
working in the federal government are heavily engaged in the process. They play a really critical

01:06:43.200 --> 01:06:49.440
role, not just using AI, but also I think helping the government develop the AI policy. And that's

01:06:49.440 --> 01:06:55.520
also going to be a very important aspect of it. And we need to have multiple platforms to

01:06:56.800 --> 01:07:02.080
facilitate closer collaborations between people in academia and also in the federal government.

01:07:02.880 --> 01:07:10.160
And this is obviously one such platform. And I think we need all to work to strengthen that.

01:07:11.200 --> 01:07:18.160
And if research statisticians just kind of talking to people in the government at the generic level,

01:07:18.160 --> 01:07:23.040
I think that the impact will be more limited. So to me, I think that statisticians at the research

01:07:23.840 --> 01:07:30.080
academic statisticians need to get to know the nitty gritty of statistical work in the federal

01:07:30.080 --> 01:07:35.280
government. You want to get into how data collected, what questions are being asked, and so on.

01:07:36.080 --> 01:07:41.040
So this way, by working together, people can develop much more powerful,

01:07:42.880 --> 01:07:47.920
understand AI methods and develop new powerful methods to really take advantage of what

01:07:47.920 --> 01:07:52.720
AI can offer. So my answer is really simple in terms of collaboration. I think the best way is

01:07:52.720 --> 01:07:57.760
to get academic research statisticians and government statisticians to get married,

01:07:57.760 --> 01:08:02.000
to live under the same roof at least for seven years, if not longer. Thank you.

01:08:08.400 --> 01:08:12.160
I'm really sorry about my voice. I think I'm going to have to ask somebody else to step in.

01:08:13.040 --> 01:08:16.480
But in the meantime, Burma, would you like to speak next?

01:08:18.240 --> 01:08:25.040
I'm so sorry, Jennifer. I hope it feels better soon. I just wanted to make a comment about

01:08:25.040 --> 01:08:33.040
education. I think that in the last one year, 8,000 AI tools have been released. And many of us are

01:08:33.040 --> 01:08:41.840
sort of swimming in our ocean of learning, and our PhDs are seeming antediluvian. And so what can we

01:08:41.840 --> 01:08:48.080
do? And I think there is a lot of effort needs to go into training and education. And I see there

01:08:48.080 --> 01:08:55.360
that universities where education is a primary force, for example, in University of Michigan,

01:08:55.360 --> 01:09:02.800
just integrating into our Canvas course site, AI tutor has been proposed. So this AI tutor

01:09:02.800 --> 01:09:08.240
is actually integrating your course notes, your lectures from the last five years,

01:09:08.240 --> 01:09:15.920
and providing personalized AI assisted tutoring to the students. And if you as an instructor,

01:09:15.920 --> 01:09:20.000
I am sleeping at midnight, the student has an exam next morning, they can still

01:09:20.880 --> 01:09:29.280
use the AI assisted tutor. So I see a tremendous potential with democratization of global knowledge

01:09:29.280 --> 01:09:35.680
using these tools in an equitable way. Just like, you know, previously before the advent of the

01:09:35.680 --> 01:09:41.200
internet and online publication system, a researcher in India had to wait four months to

01:09:41.200 --> 01:09:47.840
get a actual copy of a journal and paper. And it was really challenging to access information.

01:09:47.840 --> 01:09:54.000
But now it would be much more unequal platform. And similarly, I also think that editorial tools

01:09:54.000 --> 01:10:01.600
are going to set the platform more equally for non English speaking researchers and writers.

01:10:01.600 --> 01:10:05.840
So I do think that there are many good things on which you can collaborate. But I really want

01:10:05.840 --> 01:10:13.040
to focus on education, because I think a lot of training is going to be needed in every sector,

01:10:13.040 --> 01:10:17.200
academia, industry, government partnership will be more important than ever.

01:10:21.200 --> 01:10:26.560
I'll chime in next. And I think Braimar wins the webinar for her fantastic use of anti-deluvian.

01:10:27.520 --> 01:10:33.440
But so I wanted to kind of get back at when we were talking about like great opportunities

01:10:33.440 --> 01:10:39.680
for the use of AI in government. But I also want to kind of issue a caution. And that's

01:10:40.240 --> 01:10:46.080
the shiny toy syndrome that we see with a lot of technology in government that when a new

01:10:46.720 --> 01:10:53.280
promising technology shows up on the horizon, there's like a race in federal agencies to see

01:10:53.360 --> 01:10:57.760
who can implement it and how it can be used and to identify all these great new things,

01:10:57.760 --> 01:11:03.680
even if it's not necessarily the best use of technology. We saw this in some cases with

01:11:03.680 --> 01:11:08.320
blockchain. We saw it with some privacy enhancing technologies at various points.

01:11:08.320 --> 01:11:13.520
And I think we're going to see it with AI. In the rush to be showing that you're using

01:11:13.520 --> 01:11:18.960
this great new technology, people are going to use it for things where it may not be best suited,

01:11:18.960 --> 01:11:23.120
or there may be better alternatives out there. Or maybe they're not properly considering the

01:11:23.120 --> 01:11:29.120
risks. So I do want to kind of issue that concern and that I think it's an incredibly

01:11:29.120 --> 01:11:34.640
promising technology that when used appropriately is going to serve government incredibly well.

01:11:35.200 --> 01:11:39.760
But there's going to be incentive to use it when it maybe shouldn't be used as well.

01:11:43.440 --> 01:11:47.840
Hey, Chris, why don't you wind up this and after you speak,

01:11:47.840 --> 01:11:50.560
we can go to the questions that are in the chat. Okay.

01:11:51.520 --> 01:11:58.080
Sure, just very quickly. First, I concur with Michael that a heavy dose of moderation might

01:11:58.080 --> 01:12:04.160
be in order for many of the shiny new toys. But I also wanted to follow up very quickly on what

01:12:04.160 --> 01:12:10.240
Bremer was saying with respect to training. In a follow up to that in the executive order,

01:12:10.880 --> 01:12:15.760
section 10.2 is about increasing AI talent in government. And I think that one of the

01:12:15.760 --> 01:12:22.240
roles that Cisco agencies can provide here is really a lot of the long list of expertise

01:12:22.240 --> 01:12:26.720
and subject matter expertise that I mentioned in my top line comments. Because as we build out

01:12:27.520 --> 01:12:32.960
the workforce, we are going to need experts in Cisco analysis. It's not just computer scientists,

01:12:32.960 --> 01:12:38.080
it's not just the IT folks per se, it's social behavior researchers, it's experts in data quality

01:12:38.080 --> 01:12:43.120
and management, experts in ethics per Nancy's comments. And so I just wanted to highlight that

01:12:43.120 --> 01:12:55.280
as well. Thank you. Thanks, Chris. Yeah. So Jen, if you don't mind, I'll kick off the Q&A and you

01:12:55.280 --> 01:13:03.520
can save your voice. Okay. So we have a Q&A session set up for the next 15 to 20 minutes.

01:13:03.520 --> 01:13:09.840
If you had a burning question, go ahead and put that in the chat. So I'll go through some of these

01:13:10.560 --> 01:13:17.200
quickly. If some of them were redundant, I might skip them based on our earlier answers. But one

01:13:17.200 --> 01:13:22.160
question for any of the panelists, maybe Bremer in particular, could you

01:13:23.680 --> 01:13:33.360
give some quick comments on how government might be useful in preventing risks from occurring or

01:13:33.360 --> 01:13:38.480
being realized? So I should not only predict, but can it also prevent?

01:13:40.960 --> 01:13:48.160
So I definitely think that, you know, I think Miguel Hernandez wrote one paper,

01:13:48.160 --> 01:13:54.240
I think a few years ago, that no artificial intelligence can be real intelligence without

01:13:54.240 --> 01:14:01.200
incorporating causal inference into it. So I think that what I was trying to elude to that

01:14:01.200 --> 01:14:06.960
being able to predict an outcome with almost like very high accuracy is important for me.

01:14:06.960 --> 01:14:12.800
But what factors in the society can I change in order to change that outcome? And so for that,

01:14:12.800 --> 01:14:20.640
I really need to know what are the key players. And I need to apply all my knowledge about

01:14:20.640 --> 01:14:25.920
confounding, selection bias, missing data, measurement error, just having a prediction.

01:14:26.880 --> 01:14:34.240
Human health, where we sit, is not really equivalent in terms of who is going to click

01:14:34.240 --> 01:14:41.040
on an icon on an internet. So just being able to predict is not my ultimate goal. I want to

01:14:41.040 --> 01:14:46.560
understand the causal factors. And similarly, when I work in biology and disease ideology,

01:14:46.560 --> 01:14:52.560
I really want to know the mechanism, the mediation, the pathway, what is happening inside the body.

01:14:52.560 --> 01:14:57.920
And so I do think that these AI tools and flexible tools, just like machine learning,

01:14:57.920 --> 01:15:05.280
is going to empower us to make many of the choices we make to be more flexible and incorporate

01:15:05.280 --> 01:15:12.960
multimodal heterogeneous data. But at the end of the day, I should have a target estimate in mind,

01:15:12.960 --> 01:15:19.280
which is policy relevant, and also exposures and the pathways in mind on which I can intervene.

01:15:19.280 --> 01:15:21.520
And so that's what I was alluding to.

01:15:27.600 --> 01:15:28.100
Nancy?

01:15:31.040 --> 01:15:39.440
Yeah, another perspective on this is that for many of the statistical functions in government,

01:15:39.440 --> 01:15:47.440
they're part of a larger agency. And they're offering statistical assistance in advice and

01:15:47.440 --> 01:15:54.320
information to whatever function and mission is going on in the larger agency.

01:15:55.600 --> 01:16:03.760
So in addition to the 13 very well-known statistical agencies, there's over 100 statistical

01:16:03.760 --> 01:16:12.160
functions going on across government. And I think one of the things that can be very helpful

01:16:13.120 --> 01:16:19.120
is when you're thinking about risk, it's also like, what is the risk that's happening

01:16:20.400 --> 01:16:29.920
within the agency as well as within the world? Where's that connection? So if you think, for

01:16:29.920 --> 01:16:38.000
example, about IRS and some of the work that Statistics of Income does that informs IRS

01:16:38.000 --> 01:16:45.840
business in terms of how do we look for anomalies? What kinds of streaming data would we be looking

01:16:45.840 --> 01:16:53.520
at? How can we develop leading indicators? So from a preventive standpoint, you really want to take

01:16:53.520 --> 01:16:58.880
your predictions and turn them into leading indicators in some instances. And sometimes

01:16:58.880 --> 01:17:04.560
that involves thinking about, well, what kind of data can we be streaming? And what algorithms

01:17:04.560 --> 01:17:11.360
can we be running on that that would sort of set up alerts for changes in the environment?

01:17:13.200 --> 01:17:17.920
If you're doing immigration statistics, for example, what do you want to be monitoring out

01:17:17.920 --> 01:17:22.880
there in the world? It could be open data. It could be visa applications. It could be all kinds of

01:17:22.880 --> 01:17:33.360
things that you would want to set up that you could help your agency with as a statistician.

01:17:34.960 --> 01:17:42.160
So I wouldn't want to leave that out in terms of how do you prevent things or

01:17:43.440 --> 01:17:47.120
what are some of the ways that statistics are being applied across the whole federal

01:17:47.120 --> 01:17:52.320
government? Not just in an agency like the Census Bureau or the Bureau of Labor Statistics, but in

01:17:52.320 --> 01:17:55.920
all these many other functions where statistical work is taking place.

01:17:57.280 --> 01:18:03.600
Xiuming, go ahead. Yeah, I just want to second Nancy's point. I think it's very important.

01:18:03.600 --> 01:18:09.200
Last year when I was attending the General Assembly at the UN Statistics Commission,

01:18:09.200 --> 01:18:14.320
I just realized that the United States was maybe the only country with such a decentralized

01:18:14.320 --> 01:18:19.120
federal system where statisticians work with different agencies. Most other countries,

01:18:19.120 --> 01:18:23.600
they have a central statistics office. So I think here in this country, it's even more

01:18:23.600 --> 01:18:30.000
important that the people across agencies work together both in terms of training, education,

01:18:30.000 --> 01:18:34.640
bringing expertise, and sharing data. So maybe it's a little bit easier to share data across

01:18:34.640 --> 01:18:41.040
federal agencies. Working together, I think across agencies will really help the government

01:18:41.760 --> 01:18:47.760
do better, whether it's a prediction or whether it's trying to prevent risk. Thank you.

01:18:51.520 --> 01:18:59.440
Another question concerns how we might ensure that the free and open source AI

01:19:00.400 --> 01:19:05.200
tools that are available now stay free and open source. Does anyone have any thoughts on this?

01:19:09.040 --> 01:19:13.280
Chris, you want to go? Yeah, I can speak a little bit about this. I mean, first of all,

01:19:13.280 --> 01:19:21.360
we can't ensure that there won't be pridership leveraged over models in the future. I mean,

01:19:21.360 --> 01:19:28.320
that's in part what a great bit of the American investment in R&D is, is commercialization.

01:19:29.040 --> 01:19:36.320
However, what we can do is make good practical use of existing open resources and then it becomes

01:19:36.320 --> 01:19:41.280
path dependent, right? It's very hard to deviate once you've adopted a particular technology. It's

01:19:41.280 --> 01:19:50.080
hard to change shift. And so by early adoption, by having really practicable security hardness

01:19:50.080 --> 01:19:57.280
testing that is done in a way that the open source community can sustain and to have policies that

01:19:57.280 --> 01:20:06.640
allow us to use open source models and algorithms and to value the culture, the open culture is,

01:20:06.640 --> 01:20:13.040
I think, really primary in terms of at least federal agencies' perspective and use of this

01:20:13.040 --> 01:20:20.320
technology. I also think that the open source community has a lot to teach us in this respect

01:20:20.320 --> 01:20:26.080
because of their incredibly good metadata tracking, versioning control, et cetera,

01:20:26.080 --> 01:20:32.080
much better than many of the proprietary solutions. And it helps foster transparency,

01:20:32.080 --> 01:20:44.000
which we value. The next question, a little bit different flavor. So a lot of the audience

01:20:44.000 --> 01:20:52.240
has advanced degrees already, but probably most of them not in AI. So how do they kind of get up

01:20:52.240 --> 01:20:58.240
to speed and embrace all the changes on the horizon without going back to school?

01:21:08.240 --> 01:21:09.840
Anyone want to take the first crack at this?

01:21:13.040 --> 01:21:20.400
So I went to a seminar. It's going to be not my knowledge, but borrowed wisdom. I went to a

01:21:20.400 --> 01:21:27.040
seminar where somebody asked, is AI going to take my job? And then the panelists responded

01:21:27.040 --> 01:21:33.600
by saying that no, but someone who knows how to use these AI tools meaningfully is going to take

01:21:33.600 --> 01:21:40.960
your job. So I think that there is a difference between actually doing research and development

01:21:40.960 --> 01:21:45.600
in AI. And there are card carrying people who have been doing this forever and they're going

01:21:45.600 --> 01:21:51.840
to continue to do that. And this new event of generative AI with trillions of parameters

01:21:51.840 --> 01:21:58.080
and tuning takes a lot of GPU and computing time. And many times it's not even affordable,

01:21:58.080 --> 01:22:05.200
that level of competition for even big consortiums of universities coming together. So I do think

01:22:05.200 --> 01:22:14.000
that we are going to be there with our quintessential tools, but learn enough about AI tools and also

01:22:14.080 --> 01:22:21.280
in team science. I do think that already you can see statistical papers where prompt engineering

01:22:21.280 --> 01:22:27.840
is used because you can control the input factors and use this black box to get to the output. So

01:22:27.840 --> 01:22:34.080
how are you going to actually use basic principles of experimental design in order to do prompt

01:22:34.080 --> 01:22:40.640
design? So I do think that every quantitative discipline is going to come at it with its own

01:22:40.640 --> 01:22:46.960
spin and our skills are still going to be highly valued. But I definitely feel that we should

01:22:46.960 --> 01:22:52.640
learn about the AI tools and the technologies that's coming, even if we do not innovate each

01:22:52.640 --> 01:22:59.200
and everyone in that space. And again, I cannot emphasize more on the training piece. And Michigan

01:22:59.200 --> 01:23:07.040
has an amazing website umjai.umich.edu and also has given us these two tools in terms of our

01:23:07.040 --> 01:23:12.560
teaching, which is called MAISI, as well as UMGPT, which is our local version

01:23:12.560 --> 01:23:18.640
of chat GPT in a more secure environment to play with. We have many, many small research projects

01:23:18.640 --> 01:23:24.240
in teams so that we can learn from each other. So I think collaboration is going to be very,

01:23:24.240 --> 01:23:29.600
become very important. I think credence, coalescence, and then luminescence, that's

01:23:29.600 --> 01:23:35.600
sort of going to be the steps in order to shine in this world. But I think we have to do a lot of

01:23:35.600 --> 01:23:41.920
work now in order to learn, which is good and bad. That's great. Zumei?

01:23:43.360 --> 01:23:48.400
I don't have a good answer. Actually, I have the same problem myself, right? I have an advanced

01:23:48.400 --> 01:23:52.640
degree. I'm a professor in the university. I have trouble keeping up with what's going on.

01:23:53.200 --> 01:23:59.680
So I would just like to share my only perspective. So what I'm doing is just to broaden my

01:24:00.640 --> 01:24:07.360
circle of friends, professional friends, and talk to people outside the ones that I normally get

01:24:07.360 --> 01:24:14.240
used to. I try to attend conferences and workshops that were not strictly in my own area. By talking

01:24:14.240 --> 01:24:22.000
to more people, I got more motivated. I know which direction I look at and hopefully that will help

01:24:22.000 --> 01:24:27.360
me keep up to date. I know it's not a good answer, but this is the same challenge that I'm facing.

01:24:29.680 --> 01:24:33.840
Thank you. Go ahead.

01:24:33.840 --> 01:24:40.400
I think we often get really, really excited about training the next generation, training young

01:24:40.400 --> 01:24:44.080
children. We should be because those investments are really important and really critical.

01:24:45.120 --> 01:24:51.840
But we do need to make sure that we prioritize retooling and career advancement in emergency

01:24:52.160 --> 01:25:00.720
regardless of what it is, not just AI, for established labs. I think that NSF and others

01:25:01.600 --> 01:25:10.400
do have a mission that is driving that perspective. I mean, really, if you train

01:25:10.400 --> 01:25:15.040
just the new generation, let's say for post-docs, for example, what labs are they going to go into

01:25:15.040 --> 01:25:20.160
to deploy their skills if they don't have good mentorships that aren't skilled in this area?

01:25:22.800 --> 01:25:31.040
I'll add on. This gets at one of the topics that Nancy really addressed in her slides. In addition

01:25:31.040 --> 01:25:34.880
to making sure that we're properly training the people who are going to be using and implementing

01:25:34.880 --> 01:25:43.680
these technologies, we have to make sure that we're providing at least basic education about

01:25:43.680 --> 01:25:48.960
how these technologies work to agency leadership and the policymakers who are going to be governing

01:25:49.040 --> 01:25:55.360
this and championing this within their agencies because if they don't understand at least the

01:25:56.000 --> 01:26:02.240
rudiments of how AI works in these contexts, then governance is going to be problematic.

01:26:06.080 --> 01:26:12.320
Yeah, very, very quickly. I would just say that for those of you who are looking to upgrade your

01:26:12.320 --> 01:26:18.080
skills, there are a lot of actual training courses for people who are mid-career and

01:26:20.400 --> 01:26:28.800
more senior in the federal agencies. There's a lot that are offered online in person in DC,

01:26:28.800 --> 01:26:35.680
if you're in DC. I suggest that people kind of brush up on their skills in this area.

01:26:36.160 --> 01:26:42.720
Some of these take more or less investment or time, but they are there. I'm aware of several

01:26:42.720 --> 01:26:49.520
of them. I teach one. I don't want to pass up the opportunity to say that this series of webinars

01:26:49.520 --> 01:26:55.520
is exactly the kind of thing that people should attend all the way through. That's what we're

01:26:55.520 --> 01:27:03.040
doing is kind of helping lift up people's knowledge and skills and abilities in this area through this

01:27:03.040 --> 01:27:09.120
whole series of webinars. Stay tuned. Thank you. Thank you for the plug-in.

01:27:12.080 --> 01:27:17.280
I want to ask a question specifically about surveys and related methodology. What is the

01:27:17.280 --> 01:27:24.000
potential impact AI has on that space and what are the potential pitfalls as well?

01:27:24.000 --> 01:27:33.920
Sorry for a more technical question as we're trying to wrap up.

01:27:33.920 --> 01:27:40.800
I won't speak to the technical aspects, but I can assure you that any agency that has a new survey

01:27:40.800 --> 01:27:46.000
will still undergo an information collection request review with the Office of Management and Budget.

01:27:46.000 --> 01:27:58.240
I personally think that there could be advantages in terms of data collection and deployment of

01:27:58.240 --> 01:28:06.400
surveys, but also many times when you have massive open text boxes, then how to use mine those data.

01:28:07.440 --> 01:28:13.840
I also think that data linkage with the survey and then external data sources,

01:28:13.840 --> 01:28:21.520
there could be potential for that. I personally think that how complex survey designs are used

01:28:21.520 --> 01:28:27.280
is unfamiliar to these AI tools. They just take data as data, so I think there is a lot of opportunity

01:28:27.280 --> 01:28:33.680
of working survey researchers and these prediction tools, how they can interact together, but I do

01:28:33.680 --> 01:28:38.800
think a lot of the extraction of information and integration and linkage to other data sources

01:28:38.800 --> 01:28:46.320
may become easier. Thank you. Nancy? Yeah, very quickly, I would just give one

01:28:46.320 --> 01:28:53.360
actual use case that people can find out more about. Hubert Hamer, the head of the National

01:28:54.880 --> 01:29:02.720
Agricultural Statistics Service talked about at FCSM, which is that they were taking the

01:29:03.680 --> 01:29:11.440
usage data that they acquired through the natural language processing and turning that around very

01:29:11.440 --> 01:29:18.880
quickly and taking it back to survey respondents where they were able to target very localized

01:29:19.440 --> 01:29:27.680
research that had been done that was relevant to the respondents and to see if getting that

01:29:27.760 --> 01:29:33.840
information in a very personalized way and very local and very centered on their interest would

01:29:33.840 --> 01:29:38.800
raise the response rates. So that's another way. It's not just sort of post-data collection

01:29:38.800 --> 01:29:44.640
processing or connecting data, but it's taking it back to the respondents to raise the response

01:29:44.640 --> 01:29:53.840
rates because they can really see how the data are being used. It's time to wrap up. I wanted to

01:29:54.480 --> 01:29:59.920
thank all the panelists again and the organizers for putting together this really great session.

01:30:00.640 --> 01:30:04.800
I want to thank the audience too. We had a fantastic turnout today. So do look for

01:30:04.800 --> 01:30:11.440
announcements about more upcoming web events and hopefully a capstone in-person event in the DC

01:30:11.440 --> 01:30:19.360
area this spring. So thank you guys again. This was really fantastic. You're really a treat.

