Good afternoon, and welcome to the inaugural webinar of the webinar series that's a collaboration
between the National Institutes of Statistical Sciences and the Federal Committee on Statistical
Methodology, AI and Federal Government Uses, Potential Applications, and Issues.
This is a new webinar series that we're collaborating on that's designed to offer something for
everybody, every federal staff and supervisors at all levels, various levels of experience in AI.
We plan to feature use cases that can span agencies and departments and disciplines,
and we're going to be presenting AI use cases by modalities like text imaging and also for
challenges like targeted webinars for examining the implications of AI for ethics and confidentiality
and privacy and organizational barriers. And we do hope with this webinar series to
push ahead and see how the cutting-edge AI enhancements may be available for all of us
in the years ahead. So with that, I'm going to turn it over to David Madison for opening remarks.
I'll introduce David here. Dave is a director of NIS, associate professor and associate department
chair of statistics and data science at Cornell University. He is currently the founding and editor
chief for data science and science and an associate editor for the Journal of Econometrics and
Statistica Sinica. He is lead PI and director for many NSF-funded projects. He is an officer for the
business and economic statistics section of the American Statistical Association and a member of
the Institute of Mathematical Statistics and an international biometric society. Thank you, Dave.
Thank you so much, Jennifer. Welcome, everyone. This is the opening event in the new AI and
federal government series, and more events will be announced soon, and we hope to culminate this
series with an in-person event in the D.C. area this spring. So this timely series arose from
collaboration between the FCSM and NIS, the National Institute of Statistical Sciences,
especially out of the NIS Government Affiliates Committee and in partnership with NIS's long-term
collaborations with NASS and USDA. NIS is a 30-year-old research nonprofit founded by the
leading statistical societies at the time. Its mission is to seed, foster, and deliver cutting
edge research in the statistical and data sciences, and that's why we're here today.
NIS aims to cut across disciplinary boundaries and sector boundaries. NIS aims to, in general,
partner and support to strengthen ongoing developments across our entire profession,
and in particular to lead when there's important gaps or new opportunities arise.
NIS has a vibrant affiliates program with 50-plus members. If you haven't heard of NIS before,
I'd encourage you to check out NIS.org. We have new programs launching in the coming months,
including an upcoming themed semester research program, joint postdoc and pre-doc programs,
and a new NIS co-laboratory for seeding new projects and long-term collaborations. So I
want to thank all of our organizers, our distinguished panelists, and then the staff
for all their efforts putting this together. Thank you very much. Back to you, Jennifer.
Thank you, Dave. And as a member of the Federal Committee on Statistical Methodology,
I'll just say that the Federal Committee on Statistical Methodology, or FCSM, is an
inter-agency committee dedicated to improving the quality of the federal statistics. I think
it started off being focused on the federal statistical system and the primary federal
statistical agencies, but now we are very committed to improving the quality of all federal statistics.
So it's an exciting time for AI. I think many of you may have seen that the
White House issued a new executive order yesterday, which I think we're going to hear more about that
coming up. So what we're going to do today is have introductory perspectives from each of our five
panelists. I will be posing a set of questions to these panelists afterwards, and there will be time
at the end for audience questions. And in the interest of just moving ahead with it,
I'm going to introduce our first panelist, Chris Markham. Chris holds the current role of Senior
Statistician and Senior Science Policy Analyst in the White House Office of Management and Budget,
Office of the Chief Statistician of the United States. Previously, he served as the Assistant
Director for Open Science and Data Policy at the White House Office of Science and Technology Policy.
In that capacity, he advanced the Bidas-Harris administration priorities on restoring trusting
government through scientific integrity and evidence-based policymaking and delivering
a more equitable, accessible return on the American public's investment in federal research. Chris?
Thank you. Thank you so much, Jennifer, and thanks, of course, to NIS and to SCSM for inviting the
Office of Chief Statistician to provide some perspective on the sort of very, very fast pace
of artificial intelligence as it's interfacing with the federal statistical system in this day
and age. As Jennifer mentioned, yesterday, President Biden signed the executive order on the safe,
secure, trustworthy development and use of artificial intelligence. The order represents
really an enormous breadth of government actions that promote responsible entry into the artificial
intelligence space, not just for use in the government, but really across society and in
each of the primary sectors that have equity in artificial intelligence. I thought it would be
a good idea to level the playing field in my comments, my top-level comments, in highlighting
that the executive order sets forth consistent definitions of artificial intelligence and
artificial intelligence models really so that we are all understanding what we're talking about
because there have been many definitions out in the field, and often those definitions are speaking
past each other, and so we thought that it would be good to anchor the terms used in the EEO.
The term artificial intelligence, or AI, is used in the EEO as having the same meaning set forth
in the AI and Government Act of 2020, and that's a machine-based system that can, for a given set of
human-defined objectives, make predictions, recommendations, or decisions influencing real
or virtual environments. AI systems use machine and human-based inputs to perceive real and
virtual environments, abstract such perceptions into models through analysis in an automated
manner, and use model inference to formulate options for information or action. The EEO goes
on to also define the term AI model in a more expansive way to mean a component of an information
system that implements that artificial intelligence technology and then uses computational
statistical or machine learning techniques to produce some outputs from a given set of human
supervisor unsupervised inputs really. Now, because the definition of AI model hinges on the
deployment of the technology itself, it protects a bit against the scope creep by excluding the
traditional deductive statistical methods that we all have used for our inference.
Now, there are important equities in the federal statistical system in the executive order. First
most is how the executive order reaffirms that the federal government will ensure that the collection,
use, and retention of data is done lawfully, is secure, and mitigates against privacy and
confidentiality risks that we care quite dearly about in the federal statistical system.
The executive order also instructs agencies to make headway into the development of privacy
enhancing technologies to ensure that America's protected data remains secure from those risks
and from the risk of unlawful disclosure and use by AI systems. Provisions are fully aligned with
Title III of the Evidence Act of 2018 and with OMB's responsibilities for promulgating
regulations for secure access to confidential statistical data. Our other equities for the
federal statistical system from the executive order include how commercially available information
is obtained, quality controlled, shared, and used by federal agencies, and the EO directs OMB to
consult with the Federal Privacy Council as well as the Interagency Council on Statistical Policy
to inform potential guidance to federal agencies on ways to mitigate privacy and
confidentiality risks from agencies' activities related to commercially available information.
So this is data that we might buy and then use in an AI model and that has to conform to the same
standards that we would normally use for information quality control and dissemination.
We know that the federal system will lead in the safe, secure, and trustworthy development and use
of AI. Recognized statistical units and agencies and the principle of statistical agencies of the
federal statistical system have subject matter experts in data collection, curation, methods
development, analysis, and thought leaders in how emerging technology can best serve our constituents
and users. We're looking forward to working with agencies in the system to implement the EO within
their authorities and capacities and to leverage that expertise. I'm looking forward to the Q&A.
Thank you, Chris. Our next panelist is Dr. Xiuming He. Dr. He joined Washington University in July
2023 as the inaugural chair of the Department of Statistics and Data Science. Previously,
he served as the HC Carver Collegiate Professor of Statistics at the University of Michigan.
He is a renowned leader in the fields of robust statistics, quantile regression, Bayesian inference,
and post-selection inferences. He is also a proponent of the interdisciplinary research
in data science. Dr. He? Thank you. Thank you, Jennifer, for the nice introduction and also I'm
grateful for the opportunity to be able to share my comments. Let me first make sure I have the
four screens shared. Okay, great. For those of you who are at my age, you may still remember AI
from science fiction before the powerful computers became a reality. When I was an assistant
professor actually at the University of Illinois at Urbana-Champaign, I was among the first actually
to use MoZack, which is the first web browser developed there. Since then, computer power and
the internet have given a new life to AI. Now AI is being talked about everywhere in the public
and the private sector. Obviously, AI depends on logic and algorithms and AI advances most
when machines can learn from data and experience. Many in the world actually have considered AI as a
transformative force in the digital age. It's really becoming a very hot topic.
Just a few days ago, Oliver Chingaya, who is actually the director of African Center for
Statistics for the UN Economic Commission, also a vice president for the International Statistical
Institute, wrote a very nice piece entitled, The Future of AI in Statistics in Africa is the
continent and ready. And I also want to mention Singapore, a small island nation in Southeast
Asia that's pursuing national AI strategy in a journey to smart nation. And their action book
made it very clear that the government and business will use AI to generate economic gains and
improve lives. There's no doubt that the world looks to the United States federal government as
a leader in the AI use. The National Artificial Intelligence Advisory Committee, also known as
NAIAC, just issued its year one report a few months ago and their executive summary starts
with the United States is facing a critical moment, a moment of both significant opportunity
and complexity. Now, obviously I wish that the NAIAC has more statistical representation.
I'm not going to repeat what Chris has already talked about in terms of what AI is, because
the question I think right now is not what AI is and whether we need to embrace AI. The question is
how to turn AI into a powerful and responsible machine intelligence in federal government.
I would view AI as a very sharp knife. If you use it right, it's incredibly useful.
If not, then you can get hurt actually badly. Coming back to statistics, I would say the
statistical principles and methods are more closely related to what I call human intelligence.
Statistical thinking and statistical approaches focus more on understanding, information sharing,
experience sharing, and idealized models. When I say understanding, I really mean understanding
at different levels, including data collection, including who might have responded to a survey,
what might have been learned from domain experts, and when we need to take corrective actions,
and so on. My main point today is to say that integration of machine intelligence,
as represented by AI, and human intelligence, where statistics is really a certain part of it,
is what we really need. AI is developing really rapidly to mimic certain parts of human intelligence.
Over time, more and more human intelligence will be integrated into AI, so I believe the
statistics community can help this process, even though it's often the case that at the end,
very little credit is given back to the statisticians. If you think of a statistical
part of human intelligence as a pool of 100 different seeds, as 10 of them get absorbed
into AI, we will be and should be able to generate 20 more, because human intelligence
has unlimited potential, especially when embraced machine intelligence to help us
think further and do more. Why is statistics so important? Why is human intelligence so important?
Because machine intelligence itself is limited to what is designed to offer. When it comes to ethics,
bias, discrimination, accountability, and so on, machines are usually unaware of the causes
and consequences, so public trust will then become a major issue.
While we often hear about epic failures of AI, and we are constantly amazed at what AI can do for us,
so I would call for more emphasis on human intelligence, including foundational,
as well as interdisciplinary research and training, built on well-researched statistical
principles and methods. At the same time, more openness to machine intelligence to promote
better cooperation of statistical principles into AI. I think that this is the responsibility
of everyone in the statistics community, whether you work for the federal government or not.
Let me end my remarks by saying that human intelligence and machine intelligence must
work together hand in hand as we will dramatically quicken our journey to smart nations,
and machine can get smarter and more responsible, and humans get even smarter. Thank you for listening.
Thank you. I do like the image of humans getting even smarter, rather than some of the dystopias
where we see humans getting even less smart. Our third panelist is Dr. Mukherjee. Dr. Mukherjee is
the John D. Kaplash Distinguished University Professor and Chair of the Department of
Biostatistics, Professor, Department of Epidemiology, and the Professor of Global Health
at all at the University of Michigan's School of Public Health. It's a lot. She also serves
as the Associate Director for Quantitative Data Sciences at the University of Michigan
Rogo Cancer Center and has been engaged with the University of Michigan Precision Health,
an institution-wide presidential initiative for the last decade in various roles. As of 2023,
she holds the position of Assistant Vice President for Research for Research Data Services Strategy
and is the founding director of the University of Michigan Summer Institute on Big Data.
Her research interests include statistical methods for analysis of electronic health records,
studies of gene-environment interactions, Bayesian methods, and shrinkage estimation
with strong collaborative areas, mainly in cancer, cardiovascular diseases, reproductive health,
exposure sciences, and environmental epidemiology. Ravar, go ahead. Thank you so much, Jennifer,
and thanks to Ness and FCSM for giving this opportunity to speak on this very important
issue, which is tiering through humanity. I did not prepare slides. As you saw that
I just started as AVP of research data strategy. I have been in meetings about AI and how to
navigate AI and the challenges associated with AI across the University of Michigan campus.
What I say today is probably going to be more relevant for federal statisticians working in
the health sector because I sit in a school of public health and most of my work is around
health data. I just want to start by mentioning that the first paper on AI by Alan Turing was
way back in 1950s in terms of can machines think? This is a seminal paper. Many of us have
read and heard about this paper. Why are we talking about it today? Because since last year,
generative AI has touched every possible core of human life. Somebody, a historian of science,
was asking me that whether I should think myself to be fortunate that I lived through a pandemic
discovery of the internet and also generative AI. This is actually a confluence of factors
which define our generation in a very, very profound way. I would like to really think from
a humanitarian perspective here because I do think that we have lots and lots of good models,
certain technology. I really love theater, so I want to start my remarks with a quote
from Uncle Vania, a play by Anton Chekhov. This quote actually is coming from 1898,
lots and lots of years ago, but the fundamental question for scientists and educators is that
will those people who live after us, in the course of 100 or 200 years, will they remember
and speak kindly of us? As we think about AI, particularly sitting in a school of public health,
I have to remember that my job is not just to predict but to prevent. I think it's extremely
important to think about interpretable AI, and this is not a new concept, and also remember
our social responsibility as we use it. I am very enthusiastic about what we are seeing in
terms of case studies. For example, I work a lot with electronic health record data,
and we use a lot of heterogeneous data in order to define a phenotype, and that involves text data,
imaging data, voice messages left by patients, and it takes a lot of effort by statisticians to
come up with integration of these heterogeneous data sources to map into a phenotype, and for
each disease we have different kind of algorithms and NLP techniques, but what I have seen in the
initial phases of even defining smoking phenotype in the electronic health record using generative AI
tools has been so much more accurate than our existing algorithm. I also saw that there are
many more advances in terms of even matrix factorization, and you can imagine how many
times matrix matrices are multiplied in modern computing every day, and a fraction of reduction
of time in matrix factorization leads to humongous advantages in terms of computational time,
but I do think that AI is not here to replace. I agree completely with my colleague, former colleague
Dr. Xiuming He, who has left Michigan, but I really agree with the point that it is not here
to replace human intelligence, but really as a complementary and supporting effort to human
intelligence. I compare this with our knowledge and working operations before and after the
advent of internet. I remember as a graduate student growing up in India, I used to remember
so many formally, so many equations, and memorization was a big part of my graduate learning,
and after the internet we do not remember most of this formula. We look up Wikipedia,
and many times there are certain errors in certain Wikipedia pages, and we have the expertise to
find that. I sort of draw that parallel with AI tools that when the AI is hallucinating,
expertise will be needed, but some of the rudimentary work in terms of say report generation,
data cleaning, if we do not want to do that, then it can expedite that work. So what is my
concern? I am very positive about making things accelerated, making things supported, and some of
the work that we do not particularly find very challenging to be eliminated and taken away from
our plate. So there is more white space for creative thinking. What gets me worried about
the bias in the training data sets? I grew up in India, and most of the training data, if you look
at the data portfolio, is not coming from the unseen and the underrepresented. If we think about
data equity, and the United States may put out a mandate for equity within the United States,
but I also worry about global data equity, and I want the world data map to be represented,
but I do think that many of these problems are going to be addressed. I think there is a lot
of advantages in terms of AI tools for, I was working on a case study of simulating synthetic
data. So in terms of simulating synthetic data, preserving privacy and confidentiality, there is
a lot of literature existing in statistics, and I think that the generative AI can actually
contribute to that literature quite substantially. In terms of thinking about policy simulation,
what kind of scenarios of policy is reasonable? Because it can mine a lot of the text and
existing content, it is coming up with a lot of interesting scenarios for policy implementation,
because I work a lot on COVID models and how to do policy simulation. Automated report generation,
I see a lot of advantages of automating that work. So I think that we have to remember that
uncertainty quantification is a very key strength of statisticians and modelers,
and predictive modeling and forecasting is very important, and it is the primary task, but we have
to come up with uncertainty measures as well. And I do think that in terms of that uncertainty
quantification, we can contribute a lot. I have been asking Chad GPT answers to a question and
asking, how certain are you using the AI tool interactively to elicit its own uncertainty?
And over time, its uncertainty estimate is changing. And I'm very, very interested in
the operational what is happening under the hood. So with that, I think I'm going to be
very optimistic in terms of doing more creative stuff, and I could not agree more
with Xiuming that humans are probably going to become smarter because some of the other work
which we do not particularly want to do would be taken away from our plate.
Thank you. One thing that makes me optimistic is that people that are really active
in this area are going to help us all become smarter with AI.
Our next panelist is Michael Haas.
Oh, are you there? Yes, I see you. Michael is the Senior Advisor for Data Access and Privacy.
He is responsible for outreach and engagement with the Census Bureau's data users on issues
related to the impact of privacy protection methodologies on the accessibility and usability
of census data. Prior to joining the Census Bureau, Michael served as Director of Student
Privacy at the U.S. Department of Education. Michael is a colleague of mine on the Federal
Committee on Statistical Methodology and Chair of its Confidentiality and Data Access Committee.
He has supported numerous federal government-wide initiatives related to data privacy and
confidentiality and served as a privacy consultant to the Federal Commission on
Evidence-Based Policymaking. Michael? Great. Thank you, Jennifer, and good afternoon, everyone.
So, advances in technology often lead to new and emerging privacy and ethical concerns,
and strategies that we can develop and solutions that we can use to mitigate
these privacy and ethical concerns typically lag behind technological advances. So, we see
kind of new concerns arising in the immediate aftermath of the development of a new technology
or the implementation of new technology. And then over time, we see kind of the privacy concerns
being identified and new solutions to address those concerns being implemented.
The same is true and will be true with artificial intelligence. I think as a pattern,
the more complex and opaque a technology is when it's being introduced to society,
the thornier the privacy and ethical concerns can be to both identify and to navigate.
In the case of AI, there are several privacy and ethical concerns that we've already identified
as relevant for federal agencies and particularly for the federal statistical system.
On the privacy front, there are concerns both about privacy in the context of AI-generated
information and concerns about AI being used to attack or defeat the confidentiality protections
of data that are generated in more traditional ways that agencies release to the public.
On the AI-generated information front, when AI models are trained on confidential information,
there's substantial risk of the confidential information from those training data sets
kind of leaking in subsequent interactions with the AI models. Now, there are promising
privacy-enhancing technologies like differential privacy and others that can address these risks
of kind of confidential information leakage from training data sets, but many of these approaches
are still in the early research stages and any successful adoption of these or other privacy
enhancing technologies would require their inclusion at the training stage of AI development
rather than as kind of a post-development privacy filter, if you will. The other major concern which
I mentioned relates to AI being used in attacks on the confidentiality protections of data that
federal agencies release to the public. We know that any public release of information that's
derived from a confidential data source is going to reveal or leak a tiny bit of confidential
information in the process. That's inevitable. There's no such thing as perfect de-identification
and agencies have to select and implement disclosure avoidance mechanisms to manage
that disclosure risk in the data that they publish, and in doing so, they have to
navigate a very complex trade-off between confidentiality protection, data accuracy,
and data availability. As new kind of attack vectors on these confidentiality protections
are identified and emerge, it throws that balance off and agencies have a difficult time
identifying exactly what the existing level of disclosure risk for their data assets are
and compensating with additional or improved confidentiality protections on the data they're
releasing. Over the past several years, we've seen kind of new and emerging attack vectors
on kind of traditional disclosure limitation methods through things such as database reconstruction,
through the increasing use of convex optimization technologies on publicly released data,
and that makes agencies' jobs harder to protect the data. There is concern that AI could be
successfully leveraged in new and emerging forms of attack on confidentiality protections, so we
are going to have to be diligent in kind of figuring out, okay, how can existing and emerging
disclosure limitation techniques be implemented such to limit the vulnerability of these
mechanisms to AI-based attacks? On the ethical front, there are also concerns. There's concerns
about data quality, which have been mentioned already, transparency, and data integrity.
On the data quality front, the more that AI exists as a black box, the less confidence we can have
in our ability to know what the limitations of our data are. As statisticians, it's important
for us to know how much we can rely on the data that we are collecting and using, so to the extent
that there are inaccuracies, to the extent that there is error or bias in the data that AI are
generating, it's important for us to be able to identify those. That requires understanding the
technologies and understanding kind of the workings of these black box algorithms in many cases.
On the transparency front, a related issue is, how can we communicate these limitations in the
data to our data users? We have an obligation to convey known limitations on data accuracy,
known limitations in the form of bias, etc., to data users in order to inform their uses of our
data. If we're using AI to generate data or to process or produce data for public consumption,
and if we can't be sure that we know all of the limitations of the data, how can we
successfully convey those to the general public? Lastly, and particularly kind of in the context of
AI technologies like ChatGBT, where they're starting to take the place of search engines,
if data users are asking these AIs to answer questions that require kind of the use of
federal statistics, how can we ensure the integrity of the information that's being
presented as federal statistics? How can we make sure that the data that these AIs are serving up
to the public if they're being drawn from federal statistics? How can we ensure that they're both
authoritative and accurate? And those are kind of some thorny questions that we're going to have to
consider and find solutions to. So these are just a few of the concerns on the privacy and ethics
front. They're absolutely ones that merit our attention, and as Chris mentioned at the beginning
of our session today, yesterday's executive order actually addresses some of these. It
instructs federal agencies to research solutions to these and to related concerns. It's also likely
that as AI continues to be developed and adopted, that new concerns on the privacy and ethical
fronts are likely to be identified, and so we should remain diligent and we should continue
to review these technologies moving forward to ensure that we're able to stay kind of current on
how we identify and address these privacy and ethical concerns. Thanks.
Thank you, Michael. Before I introduce Nancy, I wanted to alert everybody to the Q&A tab at
the bottom of your screen. We have a couple of questions, and so after each of the panelists
has had their chance to provide their perspectives, I'm going to be asking a handful
of moderated questions, and then toward the end of the webinar we will go through the participant
questions. So your question should be answered then. If not, we will figure out a way to find
an answer for you. So our last panelist who will be giving her opening remarks is Dr. Nancy Potok.
Nancy Potok has over 38 years of senior-level experience in the public policy arena within
the public, private, and nonprofit sectors. Dr. Potok is a visiting fellow at RTI International
and visiting scholar at NYU. She previously served as the chief statistician of the United States
in the executive office of the president and the deputy director and was the
chief operating officer of the U.S. Census Bureau. She was the deputy undersecretary for economic
affairs at the U.S. Department of Commerce and senior vice president of North at the University
of Chicago. She served as a commissioner on the U.S. Evidence-Based Policymaking Commission
and currently chairs and serves on several boards of nonprofit organizations and academic
institutions, including this. She is a fellow of the National Academy of Public Administration
and the American Statistical Association. Nancy. Thanks, Jennifer, and thank you to Ness and FCSM
for having this webinar series. I'm going to share my screen. I do have a couple of slides that I'd
like to use to help the conversation along, so let me get this up. Let's see. I guess I have to
start the slideshow somewhere here. There we go. Yeah, so I'll just move to the next slide. What
I want to talk about actually is a more practical application. I'm going to bring it down a little
bit to ground level here in terms of thinking about people's desire to do AI projects themselves
and how you go about doing that in government successfully. I think there are several things
to consider. We're talking about some of the big ideas, some of the big considerations,
and what I'd like to do is talk about, let's see if I can advance my slides, some of the elements
of a successful AI approach, especially if you're in government. The technical fundamentals,
obviously, are key, and I think there's a lot of help out there on what are the technical
fundamentals? How do you actually put these algorithms together? I think the webinar series
will do a much deeper dive into this. Sometimes what people want to do isn't as complicated
as it seems. A lot of it, again, depends on some of these data quality issues, what you're starting
with, what you're trying to accomplish, but I think if you just jump in on one particular thing,
even if you're not in management or leadership position in your organization, you still have
to fit within an overall AI strategy. I think some of the panelists who came before me were
laying out these strategy and policy types of issues that whatever you're doing, however big
or small your project is, really has to fit in that context under the umbrella of the executive
order if you're in government. The other thing to consider, too, is your organizational culture,
because if the organization isn't ready to accept some of the things that you want to do,
you could have a heck of a time moving your project forward. Then what is the governance
structure? I think for many people in government, it's not so much that you can't conceive of a
project that you don't want to innovate. It's how do you get things done through the governance
structure? Now that we have chief data officers, there's CIOs, you need infrastructure.
That might be your biggest barrier, actually. While you learn about AI, you have to keep these
other environmental things in your mind if you're really going to get this done. Then I think Michael
spent some time talking about responsible leadership and the ethical framework within
which you work. There are equity issues. There's data quality issues. There's many things you have
to consider. I do want to just do a very short deep dive into this, but I strongly recommend
that as you're considering what you want to do in your agency, whether it's a research
project or you think you want to buy something off the shelf or you're ready to launch,
you really need to create a roadmap before you start. These are just my suggestions based on my
own experience and the experience of lots of other people that I've interacted with across government
who are working on AI projects. That is you start by having a value proposition. Why are you doing
this? What is it going to contribute? I think that goes back a little bit to what Braumer was
saying about what are people in the future going to think about this? Where is the value? Why?
You start with that. It needs to tie into whatever the mission of your agency is
and whatever you're trying to accomplish. Why does that matter? How are you going to,
the third thing, create a narrative that explains what you're trying to do so you can get some
support in your agency for doing this and get a supporting coalition together, whether it's
people in other agencies or in other parts of the agency where you need to get your hands on the
data, you need help with the infrastructure, you're going to need some money and people.
These are just very practical considerations, but if you don't think about this, your AI project
may be very well conceived, but it's not going to go anywhere. Then how do you
set up maybe a pilot project so that you can have a deliverable and show some progress,
communicate your vision, get more buy-in? I think a lot of people set out being just overwhelmed
because they're trying to, as they say, boil the ocean. The scope of what you're trying to do is
just way too big. If you really want to be successful, start with little things that you
can do that will incrementally advance your research or your particular project that you're
trying to do. It doesn't mean go super slow or don't take any risks, but I think if you have a
smaller scope, if things aren't working out very well, you have something that's manageable that
you can course correct and think about some of these other issues. I think one of the big
problems that we see is that people kind of conflate their ability to get things done in
the AI arena, the technical problems and the adaptive challenges. I think most people actually
don't run into huge problems. They're kind of fun problems, the technical ones. You can find people
who have the technical background, whether they're inside the agency in an academic setting that you
want to partner with, an organization like NISS who can bring in experts. You can tackle those
technical problems, but if you don't think about the adaptive challenges, sort of where do the
people come into this? How are we having to learn new ways of behaving? How is this changing attitudes
or behaviors? You could really run into a lot of resistance and fear. Sometimes those adaptive
challenges are actually tougher nuts to crack than the technical ones. If you're not thinking about
that as you go along, you could end up with a big surprise when people kind of resist and say,
you're taking away my job. This is where as a statistician or as somebody who's supporting
statisticians where I've always done this work my way, now you want to do it this different way and
it's going to take away my job, so I don't want you to do it. You may not hear that exactly, but
you will be hearing that as a subtext. Then Michael kind of went through some of the ethical
issues and so I won't spend a lot of time on this, but I think these are ethical issues in
statistics and they carry right over to AI. Privacy, bias, and it is like missing data is
huge in bias, so I'm glad that was brought up. Who's not included in the training data or in
other data that you might be using? This idea of transparency, especially if you want to go out and
buy data or buy a software product that's going to do some of this for you, so fairness ties into
that. Accountability, who's responsible when you're not quite getting the results that you want?
Because this is a team effort. It requires coalitions of people. You can't sort of sit
alone and do an AI project, so understanding the various roles and responsibilities. Then
I think the most important thing in some ways is explainability. How are you going to really
maintain trust in government by being able to explain a very clear language, what it is you're
doing and what the effect is? You have to not only be able to explain it to other statisticians, but
to the public because you're still going to probably be relying on public data one way or
the other. When you get to this and let's say you have all these other things kind of
tied in a neat package, you've got the technology, you've tested your technique,
you've got some support in your agency, you still have to stop and ask yourself, is this really
something we should be doing? Is this something that people in the future, again, will thank us
for or are we going to deeply regret this because of some of these fairness and bias issues or
violating privacy or really ending up with some trust issues out there between the public and
the government? It's not just can we do it, it's should we do it. If the answer is pretty iffy
and you're not sure, don't do it. You have to be prepared to stop and I think that's a really key
thing too. You could get very close to being able to do something and just say, you know what,
we shouldn't do this from an ethical standpoint. So I'm going to stop there so we can get to some
of the questions that are both the policy and kind of the statistical application and these kind of
practical things that you really have to think of if you're actually going to be doing this in a
government setting. Thank you, Nancy. And so like I said before, if you have questions for the panel,
you can put them in the Q&A, but first we have some questions from Nissen FCSM for the panel
and I forgot to say I didn't introduce myself. I'm Jennifer Parker. I'm the Director of the
Division of Research and Methodology at the National Center for Health Statistics which is
located within the Centers for Disease Control and Prevention and I am also on the Federal
Committee on Statistical Methodology. So it's been a pleasure to hear all of the perspectives
and insights from all of our panelists and you know I have a handful of questions here but I have
actually even more now that I've heard everything laid out. So my first question for the panel is
what are some of the considerations that government statisticians and managers should
be thinking about when deciding whether to use AI for a particular use case or as part of their
workflow? And I think all of you have touched on that a little bit. I do have a second question
related to this that could be answered as part of this answer. What special issues arise because
AI methods are not transparent which is a fundamental value of the federal statistical system
and federal statistics in general. I am going to let somebody volunteer to open up
and if not I will call on somebody.
All right. I'd like to hear from one of our academic colleagues.
Zhimei, would you like to take a chance at answering that?
Yeah, I will try. I think the question is what are the considerations that government statisticians
and managers should be thinking about when they are deciding whether to use AI for a particular
problem. I think that the key question for me is to think about do we really understand
what AI or the particular AI method we are using, how this is working, how it arrives at those
conclusions and whether certain human interventions, whether out of the more part of the human
intelligence, is it really needed in arriving at the solutions. So I will certainly feel
uncomfortable just blindly use whatever tool is available, whatever algorithm or platform is
available. If we feel comfortable, we understand what the AI is doing for that particular problem,
reaching that particular conclusion, then I will say yes this is a very powerful tool that we
should rely on and we may be able to get things done more efficiently and getting better insights.
So that's my quick answer.
And if you are muted, I think.
I thought I would get through today without that. Thank you very much. I see Chris,
you have your hand up. Do you have a response or a better answer?
First, a concurrence with assuming on all regardless of the tools that if you don't
understand the tools, you won't be using them. We often refer to this as the black box problem,
where we understand pretty well the inputs, although when you are talking about inputs
on the order of billions and billions of parameters, it is often very difficult.
As we know, we can often end up in situations where there are really complicated interactions
that we don't quite understand that come out. So it is really, I think, very important to have
open standards, to have the protocols themselves to be open and for the underlying algorithms
to be visible to the public and visible to interrogation. The other thing I would like
to add on this is something related that Michael had talked about, which is one of these really
difficult challenges with transparency here is in data quality. We often care a lot about
confidentiality, obviously. One of the tradeoffs, of course, is that we want to protect confidential
data. If models aren't trained on the very best in high quality data, then their outputs will reflect
that. We have to really think very carefully about, and I think this is really an existential
question for the federal statistical system, think very carefully about what the balance
and tradeoff is between what we allow models to be trained on so that we can transparently
communicate to the public the trustworthiness of the outputs when there is still a substantial
disclosure risk. Very good point about the quality of the inputs.
Ramar, I see you have your hand up. Yes, so I just wanted to make a couple of comments. One is that
I think it would be incredibly important to collaborate with industry. So at University of
Michigan, we have created a secure, our own version of UMGBT where the data inputs are actually
secure. But the underlying operating system is actually in collaboration with Microsoft.
So I think that industry partnership with industry is going to be very key in terms of creating
secure environments where we can use all of these tools and we can lead a augmented, meaningful,
purposeful life as an analyst instead of being AI driven. That's the first thing I want to mention.
The second thing is that productivity. So already by my students who use actually GitHub Copilot
in terms of filling with codes are producing work much faster than students who are resisting.
So stack exchange is their default, but how much different is it? And so why should I be
resistant to use GitHub Copilot? So for coding and translating a syntax of code from Python to R,
I think this is absolutely a very good use. You still are not going to get absolutely accurate
coding, but you can fix it. It's 80% correct. So your starting point, taking a Python optimization
routine to R is much faster. So that gives me hope, but also concern because I feel we already
produce a lot. And in the June 2023 McKinsey report says that $6.1 to $7.9 trillion dollars
added to the economy because workers will be more productive. And the projection is that by 2030,
30% of the work hours are going to become automated. So in academia, if now I am expecting
my junior faculty to produce more, I'm going to place that as a question. And then the third
thing I want to mention is sample survey and study design. Many of the assets and I think that
the national treasures in government statistics are ongoing national surveys. And many times the
AI tools just takes data as data, does not really respect the sampling design. So we need to really
work in partnership in order to respect the study design and how data are collected. And
that's very unique to our field. Thank you. Michael, I think I saw your hand up next.
Yes. So I just wanted to follow up on two things that Chris had mentioned. So the first,
I absolutely want to second what he was saying about how thorny the balancing act can be between
ensuring data quality and data protection. And I think in the AI context, that can be even
more challenging than it already is, which is not reassuring. But it's certainly something that
merits a lot of thought and consideration. The second gets back at the transparency part of the
initial question there. And I think Chris did a really good job of explaining the importance of
algorithmic transparency as part of that, kind of moving away from the black box nature of a lot
of discussions about AI. But I think it's more than just kind of like opening up the black box
and having transparency algorithms. I think the other part of this that's going to be particularly
challenging from a transparency front is going to be explaining to non-technical audiences,
but audiences that are using the data that are produced from these algorithms,
explaining to these non-technical audiences exactly what's being done to create or to process
these data. Because this is a highly complex technical domain, and it's not going to be easy
to explain, and particularly not going to be easy to explain to those who are not kind of well-versed
in this type of algorithmic decision making. So I think we're going to have our work cut out for us
on that kind of transparency front if we're going to be able to explain kind of what's
being done to the data and what the implications on data use are.
Making these problems even greater for the federal statistical system. Nancy?
Yeah, thanks. I would like to kind of posit a different thought here, maybe a little bit of
a different perspective that I don't think is being talked about much, but could be helpful.
And that's really kind of in a governance slash autonomy context of thinking about the government
and particularly statistical agencies doing AI. We want openness, we want transparency.
The agencies also want a lot of autonomy when it comes to their methodology. And I think
transparency helps with that. But if we look at some international models, for example, not
technical models, but governance models. If you look at, for example, the UK, they have a really
sophisticated oversight and evaluation capacity where they have what I would say an intermediary
between an independent statistical function off the national statistics and the public
that kind of gives the seal of approval in a non-technical way by providing this oversight.
And we don't really have that in our governance structure. I don't think OMB
and the chief statistician's office right now is set up to be able to do that the way that they
do it in the UK, because it's tied to the White House. And so that can create trust issues as
well. It's like inside the government saying, oh, trust us from the White House. And we've seen that
can create problems. So I would say, maybe we should be thinking along with the technical
solutions to these issues about maybe we need a different governance structure that can sort of
look at the validation process, have the human intervention, review these algorithms technically,
and then be able to turn around in a non-technical way and say to the public, here's what we found.
We found bias or we didn't find bias. And then you don't have to reveal all of the algorithms and
the micro data that you're using that could create privacy problems if you're using micro
data for the training data, because then you have a limited group of outside objective people
who can sort of give that good housekeeping seal of approval and say, no, this is actually okay.
So that is not part of the conversation right now, as far as I can
tell in a serious way. And I just want to put it out there as something to think about.
Thank you, Nancy. I apologize. I was telling the group I was a little bit
with. I had a touch of laryngitis over the weekend. So I think that's a good segue into
our next question, actually. Where do you see the biggest impacts for government statistics with AI?
And how can academic researchers and statistics and governments, academic researchers and
government statisticians each contribute and collaborate towards the use of AI for
government statistics? And I apologize for my voice. Michael?
Yeah. So one, I think one low-hanging fruit that I know the Census Bureau is actively looking into,
and I believe other agencies are as well, and that's using large language models to
better understand what people who are coming to our website are looking for and also better
being able to serve our information up to the general public in other kind of web environments.
And the reason for that is so we have very particular nomenclature for our data collections
and for our data products. And we refer to things as like Table P-19 or Table G-001 or things like
that. We have very regimented nomenclature for the different data products that we're releasing and
for the different attributes and variables that are included therein. And that's not the nomenclature
that your average person looking for an official statistic is actually going to use. And so there's
often kind of a translation gap in there where kind of real language questions need to be
translated into their official statistics tables kind of official nomenclature and vice versa.
So we're looking into large language models as a mechanism for better being able to kind of perform
that translation both on the input side and on the output side to better serve data up to the
people who are ultimately the consumers of it. Yeah, that's a perfect use. Nancy?
Thanks. Yeah, I just wanted to mention a pilot project that I've been working on. It was one
that we talked about at the recent FCSM conference and that was using natural language processing
models and several algorithms to find out who's using statistical data. Going beyond kind of the
downloads from the website and really being able to see by looking at scientific publications
and reading the full text of the articles and then looking at reports that agencies are putting out
and other types of things to actually find the unsighted data that's being used and the data
assets that often don't show up just if you're looking for citations. And then making it available
to both the agencies and the users to start really building coalitions around
communities of data users, whether it's health data or education data or helping agencies kind
of get a lot more breadth and depth by sharing data and looking at how researchers are linking
data from different agencies and understanding that a lot better so the agencies can actually
improve the quality of the data and start making much better connections with the user groups.
And from an equity standpoint, again, see who's missing, which researchers are not using the data,
which communities are missing there, and how can the agencies do more outreach. And then tying that
through better kind of search functions that could be algorithmically driven the way that
Amazon is so that when someone is coming to like the application portal to get access to statistical
data, they can now see a lot of usage data, they can connect with other researchers and have a much
fuller portfolio and of understanding, especially newer researchers can see them and agencies can
see it too. So I think there's a lot of practical uses out there to really help
increase the public in researchers appreciation and the value of statistical data can be
demonstrated much better, which can only help the system. I really like the focus on the external
users and the general public and making statistics a little bit more friendly for everybody.
Thank you, Jennifer. I want to add to the one big impact, I think, for government statistics
using AI is that I think the more and more government leaders will be under pressure
to ask what more and how much more AI can do in federal government. And in turn, I think that
people doing statistical work will be more appreciated. So that's a more optimistic view
of that. And I also want to come to the collaboration between academic research statisticians
and the statisticians working in the federal government. I think that obviously the statisticians
working in the federal government are heavily engaged in the process. They play a really critical
role, not just using AI, but also I think helping the government develop the AI policy. And that's
also going to be a very important aspect of it. And we need to have multiple platforms to
facilitate closer collaborations between people in academia and also in the federal government.
And this is obviously one such platform. And I think we need all to work to strengthen that.
And if research statisticians just kind of talking to people in the government at the generic level,
I think that the impact will be more limited. So to me, I think that statisticians at the research
academic statisticians need to get to know the nitty gritty of statistical work in the federal
government. You want to get into how data collected, what questions are being asked, and so on.
So this way, by working together, people can develop much more powerful,
understand AI methods and develop new powerful methods to really take advantage of what
AI can offer. So my answer is really simple in terms of collaboration. I think the best way is
to get academic research statisticians and government statisticians to get married,
to live under the same roof at least for seven years, if not longer. Thank you.
I'm really sorry about my voice. I think I'm going to have to ask somebody else to step in.
But in the meantime, Burma, would you like to speak next?
I'm so sorry, Jennifer. I hope it feels better soon. I just wanted to make a comment about
education. I think that in the last one year, 8,000 AI tools have been released. And many of us are
sort of swimming in our ocean of learning, and our PhDs are seeming antediluvian. And so what can we
do? And I think there is a lot of effort needs to go into training and education. And I see there
that universities where education is a primary force, for example, in University of Michigan,
just integrating into our Canvas course site, AI tutor has been proposed. So this AI tutor
is actually integrating your course notes, your lectures from the last five years,
and providing personalized AI assisted tutoring to the students. And if you as an instructor,
I am sleeping at midnight, the student has an exam next morning, they can still
use the AI assisted tutor. So I see a tremendous potential with democratization of global knowledge
using these tools in an equitable way. Just like, you know, previously before the advent of the
internet and online publication system, a researcher in India had to wait four months to
get a actual copy of a journal and paper. And it was really challenging to access information.
But now it would be much more unequal platform. And similarly, I also think that editorial tools
are going to set the platform more equally for non English speaking researchers and writers.
So I do think that there are many good things on which you can collaborate. But I really want
to focus on education, because I think a lot of training is going to be needed in every sector,
academia, industry, government partnership will be more important than ever.
I'll chime in next. And I think Braimar wins the webinar for her fantastic use of anti-deluvian.
But so I wanted to kind of get back at when we were talking about like great opportunities
for the use of AI in government. But I also want to kind of issue a caution. And that's
the shiny toy syndrome that we see with a lot of technology in government that when a new
promising technology shows up on the horizon, there's like a race in federal agencies to see
who can implement it and how it can be used and to identify all these great new things,
even if it's not necessarily the best use of technology. We saw this in some cases with
blockchain. We saw it with some privacy enhancing technologies at various points.
And I think we're going to see it with AI. In the rush to be showing that you're using
this great new technology, people are going to use it for things where it may not be best suited,
or there may be better alternatives out there. Or maybe they're not properly considering the
risks. So I do want to kind of issue that concern and that I think it's an incredibly
promising technology that when used appropriately is going to serve government incredibly well.
But there's going to be incentive to use it when it maybe shouldn't be used as well.
Hey, Chris, why don't you wind up this and after you speak,
we can go to the questions that are in the chat. Okay.
Sure, just very quickly. First, I concur with Michael that a heavy dose of moderation might
be in order for many of the shiny new toys. But I also wanted to follow up very quickly on what
Bremer was saying with respect to training. In a follow up to that in the executive order,
section 10.2 is about increasing AI talent in government. And I think that one of the
roles that Cisco agencies can provide here is really a lot of the long list of expertise
and subject matter expertise that I mentioned in my top line comments. Because as we build out
the workforce, we are going to need experts in Cisco analysis. It's not just computer scientists,
it's not just the IT folks per se, it's social behavior researchers, it's experts in data quality
and management, experts in ethics per Nancy's comments. And so I just wanted to highlight that
as well. Thank you. Thanks, Chris. Yeah. So Jen, if you don't mind, I'll kick off the Q&A and you
can save your voice. Okay. So we have a Q&A session set up for the next 15 to 20 minutes.
If you had a burning question, go ahead and put that in the chat. So I'll go through some of these
quickly. If some of them were redundant, I might skip them based on our earlier answers. But one
question for any of the panelists, maybe Bremer in particular, could you
give some quick comments on how government might be useful in preventing risks from occurring or
being realized? So I should not only predict, but can it also prevent?
So I definitely think that, you know, I think Miguel Hernandez wrote one paper,
I think a few years ago, that no artificial intelligence can be real intelligence without
incorporating causal inference into it. So I think that what I was trying to elude to that
being able to predict an outcome with almost like very high accuracy is important for me.
But what factors in the society can I change in order to change that outcome? And so for that,
I really need to know what are the key players. And I need to apply all my knowledge about
confounding, selection bias, missing data, measurement error, just having a prediction.
Human health, where we sit, is not really equivalent in terms of who is going to click
on an icon on an internet. So just being able to predict is not my ultimate goal. I want to
understand the causal factors. And similarly, when I work in biology and disease ideology,
I really want to know the mechanism, the mediation, the pathway, what is happening inside the body.
And so I do think that these AI tools and flexible tools, just like machine learning,
is going to empower us to make many of the choices we make to be more flexible and incorporate
multimodal heterogeneous data. But at the end of the day, I should have a target estimate in mind,
which is policy relevant, and also exposures and the pathways in mind on which I can intervene.
And so that's what I was alluding to.
Nancy?
Yeah, another perspective on this is that for many of the statistical functions in government,
they're part of a larger agency. And they're offering statistical assistance in advice and
information to whatever function and mission is going on in the larger agency.
So in addition to the 13 very well-known statistical agencies, there's over 100 statistical
functions going on across government. And I think one of the things that can be very helpful
is when you're thinking about risk, it's also like, what is the risk that's happening
within the agency as well as within the world? Where's that connection? So if you think, for
example, about IRS and some of the work that Statistics of Income does that informs IRS
business in terms of how do we look for anomalies? What kinds of streaming data would we be looking
at? How can we develop leading indicators? So from a preventive standpoint, you really want to take
your predictions and turn them into leading indicators in some instances. And sometimes
that involves thinking about, well, what kind of data can we be streaming? And what algorithms
can we be running on that that would sort of set up alerts for changes in the environment?
If you're doing immigration statistics, for example, what do you want to be monitoring out
there in the world? It could be open data. It could be visa applications. It could be all kinds of
things that you would want to set up that you could help your agency with as a statistician.
So I wouldn't want to leave that out in terms of how do you prevent things or
what are some of the ways that statistics are being applied across the whole federal
government? Not just in an agency like the Census Bureau or the Bureau of Labor Statistics, but in
all these many other functions where statistical work is taking place.
Xiuming, go ahead. Yeah, I just want to second Nancy's point. I think it's very important.
Last year when I was attending the General Assembly at the UN Statistics Commission,
I just realized that the United States was maybe the only country with such a decentralized
federal system where statisticians work with different agencies. Most other countries,
they have a central statistics office. So I think here in this country, it's even more
important that the people across agencies work together both in terms of training, education,
bringing expertise, and sharing data. So maybe it's a little bit easier to share data across
federal agencies. Working together, I think across agencies will really help the government
do better, whether it's a prediction or whether it's trying to prevent risk. Thank you.
Another question concerns how we might ensure that the free and open source AI
tools that are available now stay free and open source. Does anyone have any thoughts on this?
Chris, you want to go? Yeah, I can speak a little bit about this. I mean, first of all,
we can't ensure that there won't be pridership leveraged over models in the future. I mean,
that's in part what a great bit of the American investment in R&D is, is commercialization.
However, what we can do is make good practical use of existing open resources and then it becomes
path dependent, right? It's very hard to deviate once you've adopted a particular technology. It's
hard to change shift. And so by early adoption, by having really practicable security hardness
testing that is done in a way that the open source community can sustain and to have policies that
allow us to use open source models and algorithms and to value the culture, the open culture is,
I think, really primary in terms of at least federal agencies' perspective and use of this
technology. I also think that the open source community has a lot to teach us in this respect
because of their incredibly good metadata tracking, versioning control, et cetera,
much better than many of the proprietary solutions. And it helps foster transparency,
which we value. The next question, a little bit different flavor. So a lot of the audience
has advanced degrees already, but probably most of them not in AI. So how do they kind of get up
to speed and embrace all the changes on the horizon without going back to school?
Anyone want to take the first crack at this?
So I went to a seminar. It's going to be not my knowledge, but borrowed wisdom. I went to a
seminar where somebody asked, is AI going to take my job? And then the panelists responded
by saying that no, but someone who knows how to use these AI tools meaningfully is going to take
your job. So I think that there is a difference between actually doing research and development
in AI. And there are card carrying people who have been doing this forever and they're going
to continue to do that. And this new event of generative AI with trillions of parameters
and tuning takes a lot of GPU and computing time. And many times it's not even affordable,
that level of competition for even big consortiums of universities coming together. So I do think
that we are going to be there with our quintessential tools, but learn enough about AI tools and also
in team science. I do think that already you can see statistical papers where prompt engineering
is used because you can control the input factors and use this black box to get to the output. So
how are you going to actually use basic principles of experimental design in order to do prompt
design? So I do think that every quantitative discipline is going to come at it with its own
spin and our skills are still going to be highly valued. But I definitely feel that we should
learn about the AI tools and the technologies that's coming, even if we do not innovate each
and everyone in that space. And again, I cannot emphasize more on the training piece. And Michigan
has an amazing website umjai.umich.edu and also has given us these two tools in terms of our
teaching, which is called MAISI, as well as UMGPT, which is our local version
of chat GPT in a more secure environment to play with. We have many, many small research projects
in teams so that we can learn from each other. So I think collaboration is going to be very,
become very important. I think credence, coalescence, and then luminescence, that's
sort of going to be the steps in order to shine in this world. But I think we have to do a lot of
work now in order to learn, which is good and bad. That's great. Zumei?
I don't have a good answer. Actually, I have the same problem myself, right? I have an advanced
degree. I'm a professor in the university. I have trouble keeping up with what's going on.
So I would just like to share my only perspective. So what I'm doing is just to broaden my
circle of friends, professional friends, and talk to people outside the ones that I normally get
used to. I try to attend conferences and workshops that were not strictly in my own area. By talking
to more people, I got more motivated. I know which direction I look at and hopefully that will help
me keep up to date. I know it's not a good answer, but this is the same challenge that I'm facing.
Thank you. Go ahead.
I think we often get really, really excited about training the next generation, training young
children. We should be because those investments are really important and really critical.
But we do need to make sure that we prioritize retooling and career advancement in emergency
regardless of what it is, not just AI, for established labs. I think that NSF and others
do have a mission that is driving that perspective. I mean, really, if you train
just the new generation, let's say for post-docs, for example, what labs are they going to go into
to deploy their skills if they don't have good mentorships that aren't skilled in this area?
I'll add on. This gets at one of the topics that Nancy really addressed in her slides. In addition
to making sure that we're properly training the people who are going to be using and implementing
these technologies, we have to make sure that we're providing at least basic education about
how these technologies work to agency leadership and the policymakers who are going to be governing
this and championing this within their agencies because if they don't understand at least the
rudiments of how AI works in these contexts, then governance is going to be problematic.
Yeah, very, very quickly. I would just say that for those of you who are looking to upgrade your
skills, there are a lot of actual training courses for people who are mid-career and
more senior in the federal agencies. There's a lot that are offered online in person in DC,
if you're in DC. I suggest that people kind of brush up on their skills in this area.
Some of these take more or less investment or time, but they are there. I'm aware of several
of them. I teach one. I don't want to pass up the opportunity to say that this series of webinars
is exactly the kind of thing that people should attend all the way through. That's what we're
doing is kind of helping lift up people's knowledge and skills and abilities in this area through this
whole series of webinars. Stay tuned. Thank you. Thank you for the plug-in.
I want to ask a question specifically about surveys and related methodology. What is the
potential impact AI has on that space and what are the potential pitfalls as well?
Sorry for a more technical question as we're trying to wrap up.
I won't speak to the technical aspects, but I can assure you that any agency that has a new survey
will still undergo an information collection request review with the Office of Management and Budget.
I personally think that there could be advantages in terms of data collection and deployment of
surveys, but also many times when you have massive open text boxes, then how to use mine those data.
I also think that data linkage with the survey and then external data sources,
there could be potential for that. I personally think that how complex survey designs are used
is unfamiliar to these AI tools. They just take data as data, so I think there is a lot of opportunity
of working survey researchers and these prediction tools, how they can interact together, but I do
think a lot of the extraction of information and integration and linkage to other data sources
may become easier. Thank you. Nancy? Yeah, very quickly, I would just give one
actual use case that people can find out more about. Hubert Hamer, the head of the National
Agricultural Statistics Service talked about at FCSM, which is that they were taking the
usage data that they acquired through the natural language processing and turning that around very
quickly and taking it back to survey respondents where they were able to target very localized
research that had been done that was relevant to the respondents and to see if getting that
information in a very personalized way and very local and very centered on their interest would
raise the response rates. So that's another way. It's not just sort of post-data collection
processing or connecting data, but it's taking it back to the respondents to raise the response
rates because they can really see how the data are being used. It's time to wrap up. I wanted to
thank all the panelists again and the organizers for putting together this really great session.
I want to thank the audience too. We had a fantastic turnout today. So do look for
announcements about more upcoming web events and hopefully a capstone in-person event in the DC
area this spring. So thank you guys again. This was really fantastic. You're really a treat.
